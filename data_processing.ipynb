{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Load the GPT-2 tokenizer\n",
    "\n",
    "from transformer import SimpleTransformer\n",
    "from one_step_thoughtsformer import ThoughtsFormer\n",
    "device = torch.device(\"cuda\")\n",
    "thoughtsformer = ThoughtsFormer(thought_length=0, vocab_size= 50257, max_sequence_length=1024, verbose=False, num_layers=2).to(device)\n",
    "transformer = SimpleTransformer(vocab_size=50257, max_sequence_length=1024, num_layers=2).to(device)\n",
    "\n",
    "embeds = torch.ones(1,32,768).to(device)\n",
    "padding = torch.zeros(1,32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 50257])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thoughtsformer.forward_ppo(embeds,padding,0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TinyShakespeareDataset(Dataset):\n",
    "    def __init__(self, token_window_size, window_offset):\n",
    "        self.token_window_size = token_window_size\n",
    "        self.window_offset = window_offset\n",
    "        \n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        text = load_dataset(\"tiny_shakespeare\", split=\"train\")['text'][0]\n",
    "        self.tokens = tokenizer.encode(text, return_tensors=\"pt\")[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.tokens) - self.token_window_size) // self.window_offset + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= len(self):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "        \n",
    "        start = idx * self.window_offset\n",
    "        end = start + self.token_window_size + 1\n",
    "        \n",
    "        return self.tokens[start:end-1], self.tokens[start+1:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Tuple\n",
    "\n",
    "token_window_size = 1024\n",
    "window_offset = 256\n",
    "\n",
    "dataset: Iterable[Tuple[torch.Tensor, torch.Tensor]]  = TinyShakespeareDataset(token_window_size,window_offset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset:\n",
    "    if i[1].shape[0] != 1024:\n",
    "        print(i[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thoughtsformer loss at step 0: 7.856191158294678\n",
      "transformer loss at step 0: 9.520978927612305\n",
      "Evaluation at step 0:\n",
      "  Avg thoughtsformer eval loss: 10.3156\n",
      "  Avg transformer eval loss: 8.4069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:409: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "thought_adam = torch.optim.Adam(params = thoughtsformer.parameters(), lr=0.001)\n",
    "trans_adam = torch.optim.Adam(params = transformer.parameters(), lr=0.001)\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "thoughtsformer.train()\n",
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    mask = torch.zeros_like(x).to(device)\n",
    "    \n",
    "    # Training step\n",
    "    thoughts_logits = thoughtsformer.forward_ppo_with_tokens(x, mask.bool(), 0)[0]\n",
    "    thoughts_logits, y = thoughts_logits.squeeze(0), y.squeeze(0)\n",
    "    thought_loss = loss_fn(thoughts_logits, y)\n",
    "    \n",
    "\n",
    "    trans_logits = transformer.forward_tokens(x, mask.bool())\n",
    "    trans_logits = trans_logits.squeeze(0)\n",
    "    trans_loss = loss_fn(trans_logits, y)\n",
    "    \n",
    "    thought_adam.zero_grad()\n",
    "    thought_loss.backward()\n",
    "    thought_adam.step()\n",
    "    \n",
    "    trans_adam.zero_grad()\n",
    "    trans_loss.backward()\n",
    "    trans_adam.step()\n",
    "    \n",
    "    print(f\"thoughtsformer loss at step {idx}: {thought_loss.item()}\")\n",
    "    print(f\"transformer loss at step {idx}: {trans_loss.item()}\")\n",
    "    \n",
    "    # Evaluation step\n",
    "    if idx % 10 == 0:\n",
    "        thoughtsformer.eval()\n",
    "        transformer.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            eval_losses = {'thoughtsformer': 0, 'transformer': 0}\n",
    "            num_eval_batches = 0\n",
    "            \n",
    "            for eval_x, eval_y in test_loader:\n",
    "                eval_x = eval_x.to(device)\n",
    "                eval_y = eval_y.to(device)\n",
    "                eval_mask = torch.zeros_like(eval_x).to(device)\n",
    "                \n",
    "                # Evaluate thoughtsformer\n",
    "                eval_thoughts_logits = thoughtsformer.forward_ppo_with_tokens(eval_x, eval_mask.bool(), 0)[0]\n",
    "                eval_thoughts_logits, eval_y = eval_thoughts_logits.squeeze(0), eval_y.squeeze(0)\n",
    "                eval_thought_loss = loss_fn(eval_thoughts_logits, eval_y)\n",
    "                eval_losses['thoughtsformer'] += eval_thought_loss.item()\n",
    "                \n",
    "                # Evaluate transformer\n",
    "                eval_trans_logits = transformer.forward_tokens(eval_x, eval_mask.bool())\n",
    "                eval_trans_logits = eval_trans_logits.squeeze(0)\n",
    "                eval_trans_loss = loss_fn(eval_trans_logits, eval_y)\n",
    "                eval_losses['transformer'] += eval_trans_loss.item()\n",
    "                \n",
    "                num_eval_batches += 1\n",
    "                break\n",
    "            \n",
    "            avg_eval_losses = {k: v / num_eval_batches for k, v in eval_losses.items()}\n",
    "            print(f\"Evaluation at step {idx}:\")\n",
    "            print(f\"  Avg thoughtsformer eval loss: {avg_eval_losses['thoughtsformer']:.4f}\")\n",
    "            print(f\"  Avg transformer eval loss: {avg_eval_losses['transformer']:.4f}\")\n",
    "        \n",
    "        thoughtsformer.train()\n",
    "        transformer.train()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "batch_size = 24\n",
    "\n",
    "token_count = 301966\n",
    "token_window_size = 1000\n",
    "window_offset = 500\n",
    "\n",
    "\n",
    "class TinyShakespeareDataset(Dataset):\n",
    "    def __init__(self, token_window_size, window_offset):\n",
    "        self.token_window_size = token_window_size\n",
    "        self.window_offset = window_offset\n",
    "        \n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        dataset = load_dataset(\"tiny_shakespeare\", split=\"train\", trust_remote_code=True)\n",
    "        text = dataset[0]['text']  # It's small, just 1 million characters\n",
    "        self.tokens = self.tokenizer.encode(text, return_tensors=\"pt\")\n",
    "        self.model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "        \n",
    "        self.cached_overlap = None\n",
    "        self.cached_start = None  # Initialize cached_start\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.tokens.shape[1] - self.token_window_size) // self.window_offset + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with torch.no_grad():\n",
    "            start = idx * self.window_offset\n",
    "            end = start + self.token_window_size\n",
    "\n",
    "            # Check if we have cached overlap and can reuse it\n",
    "            if self.cached_overlap is not None and self.cached_start == start:\n",
    "                # Reuse the cached overlap (500-1000 in the previous iteration)\n",
    "                cached_part = self.cached_overlap\n",
    "                # Compute only the new part (1000-1500)\n",
    "                new_part = self.tokens[0, end:end + self.window_offset]\n",
    "                new_embeddings = self.model(new_part.unsqueeze(0)).last_hidden_state.squeeze(0)\n",
    "                \n",
    "                # Combine the cached part and the newly computed part\n",
    "                total_embeddings = torch.cat((cached_part, new_embeddings), dim=0)\n",
    "            else:\n",
    "                # Compute the embeddings for the full window (0-1000)\n",
    "                total_tokens = self.tokens[0, start:end]  # Corrected\n",
    "                total_embeddings = self.model(total_tokens.unsqueeze(0)).last_hidden_state.squeeze(0)\n",
    "                \n",
    "            # Cache the overlap (500-1000 for the next iteration)\n",
    "            self.cached_overlap = total_embeddings[-self.window_offset:]\n",
    "            self.cached_start = start + self.window_offset  # Update cached_start\n",
    "\n",
    "            # Prepare input (0-1000) and target (1-1001) tokens\n",
    "            x = total_embeddings[:self.token_window_size, :]\n",
    "            y = self.tokens[0, start + 1:end + 1]  # Target tokens (1-1001)\n",
    "\n",
    "\n",
    "        return x, y\n",
    "            \n",
    "dataset = TinyShakespeareDataset(token_window_size, window_offset)\n",
    "\n",
    "# Example to access an item and trigger print statements\n",
    "# You can call this in a loop or at specific indices to see the output\n",
    "x, y = dataset[0]  # Change the index to test other items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyShakespeareDataset2(Dataset):\n",
    "    def __init__(self):\n",
    "        output_embeddings_file = 'embeddings.pt'  # File name for the saved embeddings\n",
    "        data = torch.load(output_embeddings_file)\n",
    "\n",
    "        # Extract embeddings and targets\n",
    "        self.embeddings = data['embeddings']\n",
    "        self.targets = data['targets']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.embeddings.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx,:,:], self.targets[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bicke\\AppData\\Local\\Temp\\ipykernel_22252\\3375324087.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(output_embeddings_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "for idx, (x, y) in enumerate(TinyShakespeareDataset2()):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # 20% for testing\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | \n",
      "1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | 21 | 22 | 23 | 24 | 25 | 26 | 27 | 28 | 29 | 30 | 31 | 32 | 33 | 34 | 35 | 36 | 37 | 38 | 39 | 40 | 41 | 42 | 43 | 44 | 45 | 46 | 47 | 48 | 49 | 50 | \n",
      "51 | 52 | 53 | 54 | 55 | 56 | 57 | 58 | 59 | 60 | "
     ]
    }
   ],
   "source": [
    "output_embeddings_file = 'embeddings.pt'  # Output file name for saving embeddings\n",
    "\n",
    "\n",
    "# Preprocess and save embeddings\n",
    "all_embeddings = []\n",
    "all_targets = []\n",
    "\n",
    "for idx, (x, y) in enumerate(TinyShakespeareDataset2):\n",
    "    all_embeddings.append(x)\n",
    "    all_targets.append(y)\n",
    "    print(idx, end = \" | \")\n",
    "    if idx % 50 == 0:\n",
    "        print()\n",
    "        all_embeddings2 = torch.cat(all_embeddings, dim=0)  # Shape: (N, token_window_size, embedding_dim)\n",
    "        all_targets2 = torch.cat(all_targets, dim=0)        # Shape: (N, token_window_size)\n",
    "\n",
    "        # Save the embeddings and targets\n",
    "        torch.save({'embeddings': all_embeddings2, 'targets': all_targets2}, output_embeddings_file)\n",
    "\n",
    "\n",
    "# Concatenate all embeddings and targets\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)  # Shape: (N, token_window_size, embedding_dim)\n",
    "all_targets = torch.cat(all_targets, dim=0)        # Shape: (N, token_window_size)\n",
    "\n",
    "# Save the embeddings and targets\n",
    "torch.save({'embeddings': all_embeddings, 'targets': all_targets}, output_embeddings_file)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([481, 1000, 768])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bicke\\AppData\\Local\\Temp\\ipykernel_22252\\1003614756.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(output_embeddings_file)\n"
     ]
    }
   ],
   "source": [
    "output_embeddings_file = 'embeddings.pt'  # File name for the saved embeddings\n",
    "data = torch.load(output_embeddings_file)\n",
    "\n",
    "# Extract embeddings and targets\n",
    "embeddings = data['embeddings']\n",
    "targets = data['targets']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([481, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5962, 22307])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megatron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
