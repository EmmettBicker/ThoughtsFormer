{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def debug_print(*args, flag=False, **kwargs):\n",
    "    if flag:\n",
    "        print(*args, **kwargs)\n",
    "\n",
    "# Pytorch's   positional encoding implementaiton\n",
    "class DualPositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 512, max_thought_len: int = 4):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_embed = d_model\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.max_len, self.max_thought_len = max_len, max_thought_len\n",
    "        self.thought_position_encoding = nn.Embedding(max_thought_len+1, self.d_embed)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, thoughts_taken: int, real_token_count: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        thoughts_taken = thoughts_taken + 1\n",
    "\n",
    "        # Reshape to put thoughts per token taken on the same dimension\n",
    "        x = x[:,:real_token_count * thoughts_taken,:].view(batch_size,real_token_count, thoughts_taken, self.d_embed)\n",
    "        # Add both kinds of embeddings\n",
    "        x = x + self.pe[:,:real_token_count].unsqueeze(2)\n",
    "        x = x + self.thought_position_encoding(torch.arange(thoughts_taken)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Reshape and pad back to original size\n",
    "        x = x.view(batch_size, -1, self.d_embed)\n",
    "\n",
    "        padding_size = self.max_len - (real_token_count * thoughts_taken)\n",
    "        if padding_size > 0:\n",
    "            x = F.pad(x, (0, 0, 0, padding_size), mode='constant', value=0)\n",
    "\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class ThoughtCausalTransformer(nn.Module):\n",
    "    def __init__(self,max_thought_len, max_sequence_length, num_layers, n_head=8, d_embed=768, feed_forward_dim=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "        self.positional_encoding = DualPositionalEncoding(d_embed, dropout, max_sequence_length, max_thought_len)\n",
    "        self.layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_embed,\n",
    "            nhead = n_head,\n",
    "            dim_feedforward=feed_forward_dim,\n",
    "            dropout=dropout,\n",
    "            activation=F.gelu,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer=self.layer,\n",
    "                                                 num_layers=num_layers,\n",
    "                                                 norm=nn.LayerNorm(d_embed))\n",
    "\n",
    "\n",
    "    def generate_thoughtsformer_mask(self, thoughts_taken, real_tokens):\n",
    "\n",
    "      main_size = self.max_sequence_length\n",
    "      block_size = thoughts_taken + 1\n",
    "      n_tokens = real_tokens\n",
    "\n",
    "      # Create the main tensor and block tensor\n",
    "      causal_mask = torch.zeros((main_size, main_size))\n",
    "      block_for_thought_sequence = torch.triu(torch.ones(block_size,block_size),diagonal=0)\n",
    "\n",
    "      # List of starting indices for the diagonal blocks\n",
    "      block_starting_idxs = torch.arange(n_tokens) * block_size\n",
    "\n",
    "      for idx in block_starting_idxs:\n",
    "          causal_mask[idx:idx+block_size, idx:idx+block_size] = block_for_thought_sequence\n",
    "          causal_mask[idx, idx+1:n_tokens*block_size] = 1\n",
    "      causal_mask = causal_mask.T == 0\n",
    "\n",
    "      return causal_mask\n",
    "\n",
    "    def generate_normal_causal_mask(self, *args):\n",
    "      return torch.triu(torch.ones(self.max_sequence_length, self.max_sequence_length),diagonal=1)\n",
    "\n",
    "    def forward(self, x, padding_mask, thoughts_taken, real_token_count):\n",
    "      x = self.positional_encoding(x, thoughts_taken, real_token_count)\n",
    "      causal_mask = self.generate_thoughtsformer_mask(thoughts_taken,real_token_count).to(x.device)\n",
    "      output = self.transformer(x, mask=causal_mask, src_key_padding_mask=padding_mask)\n",
    "\n",
    "      return output\n",
    "\n",
    "\n",
    "class ThoughtsFormer(nn.Module):\n",
    "    def __init__(self, thought_length, vocab_size, max_sequence_length, num_layers, n_head=8, d_embed=768, feed_forward_dim=2048, dropout=0.1, verbose=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len, self.d_embed = max_sequence_length, d_embed\n",
    "        self.thought_length = thought_length\n",
    "        self.transformer = ThoughtCausalTransformer(thought_length, max_sequence_length, num_layers, n_head, d_embed, feed_forward_dim, dropout)\n",
    "        self.out = nn.Linear(d_embed, vocab_size)\n",
    "        self.debug = verbose\n",
    "\n",
    "    def forward(self, embeddings, padding_mask):\n",
    "        self.token_positions = torch.where(padding_mask == False)[1]\n",
    "        self.n_real_tokens = int(self.seq_len - torch.sum(padding_mask))\n",
    "\n",
    "        original_embeddings = embeddings\n",
    "        debug_print(\"original embeddings \\n\", embeddings, flag=self.debug)\n",
    "\n",
    "        for thoughts_taken in range(self.thought_length+1):\n",
    "          next_embeddings = self.transformer(embeddings, padding_mask, thoughts_taken, self.n_real_tokens)\n",
    "          # next_embeddings = torch.arange(self.seq_len).view(1,-1,1).repeat([embeddings.size(0),1,d_embed])\n",
    "          debug_print(next_embeddings, next_embeddings.shape, flag=self.debug)\n",
    "          if thoughts_taken != self.thought_length: # Don't need to insert next thoughts if there's not going to be another iteration\n",
    "            embeddings = self.insert_thoughts(next_embeddings, original_embeddings, thoughts_taken + 1)\n",
    "          debug_print(\"updated embeddings\\n\", embeddings,  flag=self.debug)\n",
    "\n",
    "          original_embeddings = embeddings\n",
    "\n",
    "        return self.out(embeddings) # logits for everything\n",
    "\n",
    "    def forward_ppo(self, embeddings, padding_mask, thoughts_taken):\n",
    "        self.token_positions = torch.where(padding_mask == False)[1]\n",
    "        self.n_real_tokens = int(self.seq_len - torch.sum(padding_mask))\n",
    "\n",
    "        \n",
    "        next_embeddings = self.transformer(embeddings, padding_mask, thoughts_taken, self.n_real_tokens)\n",
    "\n",
    "        if thoughts_taken != self.thought_length: # Don't need to insert next thoughts if there's not going to be another iteration\n",
    "          embeddings = self.insert_thoughts(next_embeddings, embeddings, thoughts_taken + 1)\n",
    "\n",
    "        return embeddings # logits for everything\n",
    "\n",
    "\n",
    "    def insert_thoughts(self, next_embeddings, original_embeddings, iter):\n",
    "\n",
    "      debug_print(\"Debugging here \", self.n_real_tokens, self.token_positions.size(0), flag=self.debug)\n",
    "      n_elements = self.token_positions.size(0) * iter\n",
    "      n_element_next = self.token_positions.size(0) * (iter + 1)\n",
    "      batch_size, seq_len, d_embed = original_embeddings.shape\n",
    "      # we'll reshape and concat\n",
    "      # to go from\n",
    "      # 1, t            # 1, t, t\n",
    "      # 2, t    --->    # 2, t, t\n",
    "      # 3, t.flatten()  # 3, t, t.flatten()\n",
    "\n",
    "      original_embeddings = original_embeddings[:,:n_elements,:].view(batch_size,-1, iter, d_embed)\n",
    "\n",
    "\n",
    "      # This gets the positions of the next tokens to predict - 1, so right before the tokens that are being predicted\n",
    "      next_token_positions = (torch.arange(self.token_positions.size(0)) + 1) * iter - 1\n",
    "      next_embeddings = next_embeddings[:, next_token_positions,:]\n",
    "\n",
    "      # Reshapes the embeddings so they can be concatenated like in the previous diagram\n",
    "      next_embeddings = next_embeddings.view(next_embeddings.size(0), next_embeddings.size(1), 1, next_embeddings.size(2))\n",
    "\n",
    "      #Concatenates and reshapes back\n",
    "      final_embeds = torch.cat((original_embeddings,next_embeddings),dim=2)\n",
    "      final_embeds = final_embeds.view(batch_size,-1,d_embed)\n",
    "      debug_print(\"final embedding shape\", flag=self.debug)\n",
    "      debug_print(final_embeds.shape, seq_len, n_element_next, flag=self.debug)\n",
    "      padding = torch.zeros(final_embeds.size(0), seq_len-n_element_next, final_embeds.size(2))\n",
    "      final_embeds = torch.cat((final_embeds, padding),dim=1)\n",
    "\n",
    "      self.token_positions = self.get_next_token_count(self.token_positions)\n",
    "\n",
    "      return final_embeds\n",
    "\n",
    "\n",
    "\n",
    "    def get_next_token_count(self, token_positions):\n",
    "      '''\n",
    "      Updates the internal token_positions variable. Assumes each thought train will have the same length.\n",
    "      '''\n",
    "      return token_positions + torch.arange(self.token_positions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megatron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
