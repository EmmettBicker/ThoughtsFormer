{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from one_step_thoughtsformer import ThoughtsFormer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.44M\n",
      "['lm_head.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.ln_f.bias', 'transformer.ln_f.weight', 'transformer.wpe.weight', 'transformer.wte.weight']\n",
      "['lm_head.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.ln_f.bias', 'transformer.ln_f.weight', 'transformer.wpe.weight', 'transformer.wte.weight']\n"
     ]
    }
   ],
   "source": [
    "from x import GPT\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# create a from-scratch initialized minGPT model\n",
    "config = GPT.get_default_config()\n",
    "config.model_type = \"gpt2\"\n",
    "config.vocab_size = 50257 # openai's model vocabulary\n",
    "config.block_size = 1024  # openai's model block_size\n",
    "model = GPT(config)\n",
    "sd = model.state_dict()\n",
    "\n",
    "sd = {x: y for x, y in sd.items() if \".attn.bias\" not in x}\n",
    "# init a huggingface/transformers model\n",
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "# copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')] # ignore these\n",
    "transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "# basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla nn.Linear.\n",
    "# this means that we have to transpose these weights when we import them\n",
    "print(sorted(keys))\n",
    "print(sorted(sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.44M\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer\n\u001b[0;32m      5\u001b[0m t \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-medium\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bicke\\Documents\\GitHub\\Thoughtsformer\\x.py:199\u001b[0m, in \u001b[0;36mGPT.from_pretrained\u001b[1;34m(cls, model_type)\u001b[0m\n\u001b[0;32m    196\u001b[0m transposed \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn.c_attn.weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp.c_fc.weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp.c_proj.weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla nn.Linear.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# this means that we have to transpose these weights when we import them\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(keys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(sd)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(k\u001b[38;5;241m.\u001b[39mendswith(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m transposed):\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# special treatment for the Conv1D weights we need to transpose\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from x import GPT\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "t = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "next_token = \"\"\n",
    "sentence = \"Once upon a time, \"\n",
    "inp_tokens = t.encode(sentence,  return_tensors='pt')\n",
    "for i in range(10):\n",
    "    sentence += next_token\n",
    "    \n",
    "    prediction = model(inp_tokens)[0][:,-1,:].argmax(dim=-1).unsqueeze(-1)\n",
    " \n",
    "    # print(tokens.shape)\n",
    "    # predictions = model(tokens)[0]\n",
    "    # print(predictions)\n",
    "    # x = predictions.argmax(dim=-1)\n",
    "\n",
    "    inp_tokens = torch.cat([inp_tokens, prediction], dim=-1)\n",
    "\n",
    "    print(t.decode(inp_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a land far away, the\n",
      "Once upon a time, in a land far away, the world\n",
      "Once upon a time, in a land far away, the world was\n",
      "Once upon a time, in a land far away, the world was a\n",
      "Once upon a time, in a land far away, the world was a land\n",
      "Once upon a time, in a land far away, the world was a land of\n",
      "Once upon a time, in a land far away, the world was a land of peace\n",
      "Once upon a time, in a land far away, the world was a land of peace and\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony.\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony.\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony.\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony.\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony\n",
      "Once upon a time, in a land far away, the world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony. The world was a land of peace and harmony.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").eval()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def generate(prompt, max_length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "        next_token_id = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(-1)\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "        print(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "generate(\"Once upon a time, in a land far away,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.dual_positional_encoding.learned_positional_encoding.weight',\n",
       " 'transformer.dual_positional_encoding.position_in_thought_encoding.weight',\n",
       " 'transformer.transformer.layers.0.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.0.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.0.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.0.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.0.linear1.weight',\n",
       " 'transformer.transformer.layers.0.linear1.bias',\n",
       " 'transformer.transformer.layers.0.linear2.weight',\n",
       " 'transformer.transformer.layers.0.linear2.bias',\n",
       " 'transformer.transformer.layers.0.norm1.weight',\n",
       " 'transformer.transformer.layers.0.norm1.bias',\n",
       " 'transformer.transformer.layers.0.norm2.weight',\n",
       " 'transformer.transformer.layers.0.norm2.bias',\n",
       " 'transformer.transformer.layers.1.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.1.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.1.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.1.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.1.linear1.weight',\n",
       " 'transformer.transformer.layers.1.linear1.bias',\n",
       " 'transformer.transformer.layers.1.linear2.weight',\n",
       " 'transformer.transformer.layers.1.linear2.bias',\n",
       " 'transformer.transformer.layers.1.norm1.weight',\n",
       " 'transformer.transformer.layers.1.norm1.bias',\n",
       " 'transformer.transformer.layers.1.norm2.weight',\n",
       " 'transformer.transformer.layers.1.norm2.bias',\n",
       " 'transformer.transformer.layers.2.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.2.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.2.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.2.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.2.linear1.weight',\n",
       " 'transformer.transformer.layers.2.linear1.bias',\n",
       " 'transformer.transformer.layers.2.linear2.weight',\n",
       " 'transformer.transformer.layers.2.linear2.bias',\n",
       " 'transformer.transformer.layers.2.norm1.weight',\n",
       " 'transformer.transformer.layers.2.norm1.bias',\n",
       " 'transformer.transformer.layers.2.norm2.weight',\n",
       " 'transformer.transformer.layers.2.norm2.bias',\n",
       " 'transformer.transformer.norm.weight',\n",
       " 'transformer.transformer.norm.bias',\n",
       " 'policy_feedforward.weight',\n",
       " 'policy_feedforward.bias',\n",
       " 'value_feedforward.0.weight',\n",
       " 'value_feedforward.0.bias',\n",
       " 'value_feedforward.2.weight',\n",
       " 'value_feedforward.2.bias',\n",
       " 'token_embedding.weight']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from one_step_thoughtsformer import ThoughtsFormer\n",
    "import torch\n",
    "import math\n",
    "\n",
    "max_context = 10\n",
    "batch_size, max_seq_len, d_embed = 2, 20, 6\n",
    "thought_length = 1\n",
    "seq_len = 3\n",
    "thoughts_this_time = 1\n",
    "\n",
    "a = torch.arange(seq_len * (thoughts_this_time+1)).view(seq_len * (thoughts_this_time+1),1).repeat(1,d_embed)\n",
    "b = F.pad(torch.arange(2).view(2,1).repeat(1,d_embed), (0,0,0,4))\n",
    "test_embeds = torch.stack((a,b), dim=0)\n",
    "# test_embeds = torch.ones(batch_size, (thoughts_this_time+1)*seq_len,d_embed)\n",
    "# print(test_embeds)\n",
    "# y = simple_batched_reshape_with_offset(test_embeds,seq_length=5, offset=thoughts_this_time+1,row_length=thought_length + 1, pad_value=0)\n",
    "# print(y, y.shape)\n",
    "\n",
    "\n",
    "# print(test_embeds.shape)\n",
    "padding_mask = torch.zeros(batch_size, 5)\n",
    "x = ThoughtsFormer(thought_length=thought_length, vocab_size=30000, max_context_length=max_context, d_embed=6,n_head=2, num_layers=3, verbose=False, sinusoidal_position_encoding=False,dropout=0)\n",
    "[s for s in x.state_dict().keys()]\n",
    "# x.forward_ppo(test_embeds,padding_mask,thoughts_this_time)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head.weight\n",
      "transformer.h.0.attn.c_attn.bias\n",
      "transformer.h.0.attn.c_attn.weight\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.ln_1.bias\n",
      "transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_2.bias\n",
      "transformer.h.0.ln_2.weight\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.1.attn.c_attn.bias\n",
      "transformer.h.1.attn.c_attn.weight\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.ln_1.bias\n",
      "transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_2.bias\n",
      "transformer.h.1.ln_2.weight\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.10.attn.c_attn.bias\n",
      "transformer.h.10.attn.c_attn.weight\n",
      "transformer.h.10.attn.c_proj.bias\n",
      "transformer.h.10.attn.c_proj.weight\n",
      "transformer.h.10.ln_1.bias\n",
      "transformer.h.10.ln_1.weight\n",
      "transformer.h.10.ln_2.bias\n",
      "transformer.h.10.ln_2.weight\n",
      "transformer.h.10.mlp.c_fc.bias\n",
      "transformer.h.10.mlp.c_fc.weight\n",
      "transformer.h.10.mlp.c_proj.bias\n",
      "transformer.h.10.mlp.c_proj.weight\n",
      "transformer.h.11.attn.c_attn.bias\n",
      "transformer.h.11.attn.c_attn.weight\n",
      "transformer.h.11.attn.c_proj.bias\n",
      "transformer.h.11.attn.c_proj.weight\n",
      "transformer.h.11.ln_1.bias\n",
      "transformer.h.11.ln_1.weight\n",
      "transformer.h.11.ln_2.bias\n",
      "transformer.h.11.ln_2.weight\n",
      "transformer.h.11.mlp.c_fc.bias\n",
      "transformer.h.11.mlp.c_fc.weight\n",
      "transformer.h.11.mlp.c_proj.bias\n",
      "transformer.h.11.mlp.c_proj.weight\n",
      "transformer.h.2.attn.c_attn.bias\n",
      "transformer.h.2.attn.c_attn.weight\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.ln_1.bias\n",
      "transformer.h.2.ln_1.weight\n",
      "transformer.h.2.ln_2.bias\n",
      "transformer.h.2.ln_2.weight\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.3.attn.c_attn.bias\n",
      "transformer.h.3.attn.c_attn.weight\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.ln_1.bias\n",
      "transformer.h.3.ln_1.weight\n",
      "transformer.h.3.ln_2.bias\n",
      "transformer.h.3.ln_2.weight\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.4.attn.c_attn.bias\n",
      "transformer.h.4.attn.c_attn.weight\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.ln_1.bias\n",
      "transformer.h.4.ln_1.weight\n",
      "transformer.h.4.ln_2.bias\n",
      "transformer.h.4.ln_2.weight\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.5.attn.c_attn.bias\n",
      "transformer.h.5.attn.c_attn.weight\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.ln_1.bias\n",
      "transformer.h.5.ln_1.weight\n",
      "transformer.h.5.ln_2.bias\n",
      "transformer.h.5.ln_2.weight\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.6.attn.c_attn.bias\n",
      "transformer.h.6.attn.c_attn.weight\n",
      "transformer.h.6.attn.c_proj.bias\n",
      "transformer.h.6.attn.c_proj.weight\n",
      "transformer.h.6.ln_1.bias\n",
      "transformer.h.6.ln_1.weight\n",
      "transformer.h.6.ln_2.bias\n",
      "transformer.h.6.ln_2.weight\n",
      "transformer.h.6.mlp.c_fc.bias\n",
      "transformer.h.6.mlp.c_fc.weight\n",
      "transformer.h.6.mlp.c_proj.bias\n",
      "transformer.h.6.mlp.c_proj.weight\n",
      "transformer.h.7.attn.c_attn.bias\n",
      "transformer.h.7.attn.c_attn.weight\n",
      "transformer.h.7.attn.c_proj.bias\n",
      "transformer.h.7.attn.c_proj.weight\n",
      "transformer.h.7.ln_1.bias\n",
      "transformer.h.7.ln_1.weight\n",
      "transformer.h.7.ln_2.bias\n",
      "transformer.h.7.ln_2.weight\n",
      "transformer.h.7.mlp.c_fc.bias\n",
      "transformer.h.7.mlp.c_fc.weight\n",
      "transformer.h.7.mlp.c_proj.bias\n",
      "transformer.h.7.mlp.c_proj.weight\n",
      "transformer.h.8.attn.c_attn.bias\n",
      "transformer.h.8.attn.c_attn.weight\n",
      "transformer.h.8.attn.c_proj.bias\n",
      "transformer.h.8.attn.c_proj.weight\n",
      "transformer.h.8.ln_1.bias\n",
      "transformer.h.8.ln_1.weight\n",
      "transformer.h.8.ln_2.bias\n",
      "transformer.h.8.ln_2.weight\n",
      "transformer.h.8.mlp.c_fc.bias\n",
      "transformer.h.8.mlp.c_fc.weight\n",
      "transformer.h.8.mlp.c_proj.bias\n",
      "transformer.h.8.mlp.c_proj.weight\n",
      "transformer.h.9.attn.c_attn.bias\n",
      "transformer.h.9.attn.c_attn.weight\n",
      "transformer.h.9.attn.c_proj.bias\n",
      "transformer.h.9.attn.c_proj.weight\n",
      "transformer.h.9.ln_1.bias\n",
      "transformer.h.9.ln_1.weight\n",
      "transformer.h.9.ln_2.bias\n",
      "transformer.h.9.ln_2.weight\n",
      "transformer.h.9.mlp.c_fc.bias\n",
      "transformer.h.9.mlp.c_fc.weight\n",
      "transformer.h.9.mlp.c_proj.bias\n",
      "transformer.h.9.mlp.c_proj.weight\n",
      "transformer.ln_f.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.wpe.weight\n",
      "transformer.wte.weight\n"
     ]
    }
   ],
   "source": [
    "print(*[s for s in sorted(keys)], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.dual_positional_encoding.learned_positional_encoding.weight',\n",
       " 'transformer.transformer.layers.0.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.0.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.0.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.0.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.0.linear1.weight',\n",
       " 'transformer.transformer.layers.0.linear1.bias',\n",
       " 'transformer.transformer.layers.0.linear2.weight',\n",
       " 'transformer.transformer.layers.0.linear2.bias',\n",
       " 'transformer.transformer.layers.0.norm1.weight',\n",
       " 'transformer.transformer.layers.0.norm1.bias',\n",
       " 'transformer.transformer.layers.0.norm2.weight',\n",
       " 'transformer.transformer.layers.0.norm2.bias',\n",
       " 'transformer.transformer.layers.1.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.1.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.1.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.1.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.1.linear1.weight',\n",
       " 'transformer.transformer.layers.1.linear1.bias',\n",
       " 'transformer.transformer.layers.1.linear2.weight',\n",
       " 'transformer.transformer.layers.1.linear2.bias',\n",
       " 'transformer.transformer.layers.1.norm1.weight',\n",
       " 'transformer.transformer.layers.1.norm1.bias',\n",
       " 'transformer.transformer.layers.1.norm2.weight',\n",
       " 'transformer.transformer.layers.1.norm2.bias',\n",
       " 'transformer.transformer.layers.2.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.2.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.2.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.2.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.2.linear1.weight',\n",
       " 'transformer.transformer.layers.2.linear1.bias',\n",
       " 'transformer.transformer.layers.2.linear2.weight',\n",
       " 'transformer.transformer.layers.2.linear2.bias',\n",
       " 'transformer.transformer.layers.2.norm1.weight',\n",
       " 'transformer.transformer.layers.2.norm1.bias',\n",
       " 'transformer.transformer.layers.2.norm2.weight',\n",
       " 'transformer.transformer.layers.2.norm2.bias',\n",
       " 'transformer.transformer.layers.3.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.3.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.3.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.3.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.3.linear1.weight',\n",
       " 'transformer.transformer.layers.3.linear1.bias',\n",
       " 'transformer.transformer.layers.3.linear2.weight',\n",
       " 'transformer.transformer.layers.3.linear2.bias',\n",
       " 'transformer.transformer.layers.3.norm1.weight',\n",
       " 'transformer.transformer.layers.3.norm1.bias',\n",
       " 'transformer.transformer.layers.3.norm2.weight',\n",
       " 'transformer.transformer.layers.3.norm2.bias',\n",
       " 'transformer.transformer.layers.4.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.4.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.4.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.4.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.4.linear1.weight',\n",
       " 'transformer.transformer.layers.4.linear1.bias',\n",
       " 'transformer.transformer.layers.4.linear2.weight',\n",
       " 'transformer.transformer.layers.4.linear2.bias',\n",
       " 'transformer.transformer.layers.4.norm1.weight',\n",
       " 'transformer.transformer.layers.4.norm1.bias',\n",
       " 'transformer.transformer.layers.4.norm2.weight',\n",
       " 'transformer.transformer.layers.4.norm2.bias',\n",
       " 'transformer.transformer.layers.5.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.5.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.5.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.5.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.5.linear1.weight',\n",
       " 'transformer.transformer.layers.5.linear1.bias',\n",
       " 'transformer.transformer.layers.5.linear2.weight',\n",
       " 'transformer.transformer.layers.5.linear2.bias',\n",
       " 'transformer.transformer.layers.5.norm1.weight',\n",
       " 'transformer.transformer.layers.5.norm1.bias',\n",
       " 'transformer.transformer.layers.5.norm2.weight',\n",
       " 'transformer.transformer.layers.5.norm2.bias',\n",
       " 'transformer.transformer.layers.6.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.6.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.6.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.6.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.6.linear1.weight',\n",
       " 'transformer.transformer.layers.6.linear1.bias',\n",
       " 'transformer.transformer.layers.6.linear2.weight',\n",
       " 'transformer.transformer.layers.6.linear2.bias',\n",
       " 'transformer.transformer.layers.6.norm1.weight',\n",
       " 'transformer.transformer.layers.6.norm1.bias',\n",
       " 'transformer.transformer.layers.6.norm2.weight',\n",
       " 'transformer.transformer.layers.6.norm2.bias',\n",
       " 'transformer.transformer.layers.7.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.7.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.7.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.7.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.7.linear1.weight',\n",
       " 'transformer.transformer.layers.7.linear1.bias',\n",
       " 'transformer.transformer.layers.7.linear2.weight',\n",
       " 'transformer.transformer.layers.7.linear2.bias',\n",
       " 'transformer.transformer.layers.7.norm1.weight',\n",
       " 'transformer.transformer.layers.7.norm1.bias',\n",
       " 'transformer.transformer.layers.7.norm2.weight',\n",
       " 'transformer.transformer.layers.7.norm2.bias',\n",
       " 'transformer.transformer.layers.8.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.8.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.8.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.8.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.8.linear1.weight',\n",
       " 'transformer.transformer.layers.8.linear1.bias',\n",
       " 'transformer.transformer.layers.8.linear2.weight',\n",
       " 'transformer.transformer.layers.8.linear2.bias',\n",
       " 'transformer.transformer.layers.8.norm1.weight',\n",
       " 'transformer.transformer.layers.8.norm1.bias',\n",
       " 'transformer.transformer.layers.8.norm2.weight',\n",
       " 'transformer.transformer.layers.8.norm2.bias',\n",
       " 'transformer.transformer.layers.9.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.9.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.9.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.9.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.9.linear1.weight',\n",
       " 'transformer.transformer.layers.9.linear1.bias',\n",
       " 'transformer.transformer.layers.9.linear2.weight',\n",
       " 'transformer.transformer.layers.9.linear2.bias',\n",
       " 'transformer.transformer.layers.9.norm1.weight',\n",
       " 'transformer.transformer.layers.9.norm1.bias',\n",
       " 'transformer.transformer.layers.9.norm2.weight',\n",
       " 'transformer.transformer.layers.9.norm2.bias',\n",
       " 'transformer.transformer.layers.10.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.10.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.10.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.10.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.10.linear1.weight',\n",
       " 'transformer.transformer.layers.10.linear1.bias',\n",
       " 'transformer.transformer.layers.10.linear2.weight',\n",
       " 'transformer.transformer.layers.10.linear2.bias',\n",
       " 'transformer.transformer.layers.10.norm1.weight',\n",
       " 'transformer.transformer.layers.10.norm1.bias',\n",
       " 'transformer.transformer.layers.10.norm2.weight',\n",
       " 'transformer.transformer.layers.10.norm2.bias',\n",
       " 'transformer.transformer.layers.11.self_attn.in_proj_weight',\n",
       " 'transformer.transformer.layers.11.self_attn.in_proj_bias',\n",
       " 'transformer.transformer.layers.11.self_attn.out_proj.weight',\n",
       " 'transformer.transformer.layers.11.self_attn.out_proj.bias',\n",
       " 'transformer.transformer.layers.11.linear1.weight',\n",
       " 'transformer.transformer.layers.11.linear1.bias',\n",
       " 'transformer.transformer.layers.11.linear2.weight',\n",
       " 'transformer.transformer.layers.11.linear2.bias',\n",
       " 'transformer.transformer.layers.11.norm1.weight',\n",
       " 'transformer.transformer.layers.11.norm1.bias',\n",
       " 'transformer.transformer.layers.11.norm2.weight',\n",
       " 'transformer.transformer.layers.11.norm2.bias',\n",
       " 'transformer.transformer.norm.weight',\n",
       " 'transformer.transformer.norm.bias',\n",
       " 'policy_feedforward.weight',\n",
       " 'token_embedding.weight']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thoughtsformer = ThoughtsFormer(num_layers=12,vocab_size=50257, d_embed = 768, feed_forward_dim=768 * 4, n_head=12, max_context_length=1024, thought_length=0,sinusoidal_position_encoding=False)\n",
    "[s for s in thoughtsformer.state_dict().keys() if \"transformer.dual_positional_encoding.position_in_thought_encoding.weight\" != s and \"value\" not in s]\n",
    "\n",
    "\n",
    "# mapping = {\n",
    "#     'transformer.dual_positional_encoding.learned_positional_encoding.weight': 'transformer.wpe.weight',\n",
    "#     'token_embedding.weight' : 'transformer.wte.weight',\n",
    "#      'transformer.transformer.norm.weight' : 'transformer.ln_f.weight',\n",
    "#      'transformer.transformer.norm.bias' : 'transformer.ln_f.bias'\n",
    "# }\n",
    "\n",
    "# for i in range(12):\n",
    "#     mapping[f'transformer.transformer.layers.{i}.self_attn.in_proj_weight'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [3],\n",
       "         [2],\n",
       "         [4],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([\n",
    "    [1,2,0,0],\n",
    "    [3,4,0,0]]).unsqueeze(0).unsqueeze(3).permute(0,2,1,3)\n",
    "\n",
    "a.reshape(1,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ThoughtsFormer\n",
      "==================================================\n",
      "Layer: transformer.positional_encoding.thought_position_encoding.weight\n",
      "Shape: torch.Size([1, 768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.layer.self_attn.in_proj_weight\n",
      "Shape: torch.Size([2304, 768])\n",
      "Number of parameters: 1769472\n",
      "------------------------------\n",
      "Layer: transformer.layer.self_attn.in_proj_bias\n",
      "Shape: torch.Size([2304])\n",
      "Number of parameters: 2304\n",
      "------------------------------\n",
      "Layer: transformer.layer.self_attn.out_proj.weight\n",
      "Shape: torch.Size([768, 768])\n",
      "Number of parameters: 589824\n",
      "------------------------------\n",
      "Layer: transformer.layer.self_attn.out_proj.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.layer.linear1.weight\n",
      "Shape: torch.Size([2048, 768])\n",
      "Number of parameters: 1572864\n",
      "------------------------------\n",
      "Layer: transformer.layer.linear1.bias\n",
      "Shape: torch.Size([2048])\n",
      "Number of parameters: 2048\n",
      "------------------------------\n",
      "Layer: transformer.layer.linear2.weight\n",
      "Shape: torch.Size([768, 2048])\n",
      "Number of parameters: 1572864\n",
      "------------------------------\n",
      "Layer: transformer.layer.linear2.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.layer.norm1.weight\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.layer.norm1.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.layer.norm2.weight\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.layer.norm2.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.0.self_attn.in_proj_weight\n",
      "Shape: torch.Size([2304, 768])\n",
      "Number of parameters: 1769472\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.0.self_attn.in_proj_bias\n",
      "Shape: torch.Size([2304])\n",
      "Number of parameters: 2304\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.0.self_attn.out_proj.weight\n",
      "Shape: torch.Size([768, 768])\n",
      "Number of parameters: 589824\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.0.self_attn.out_proj.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.0.linear1.weight\n",
      "Shape: torch.Size([2048, 768])\n",
      "Number of parameters: 1572864\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.0.linear1.bias\n",
      "Shape: torch.Size([2048])\n",
      "Number of parameters: 2048\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.0.linear2.weight\n",
      "Shape: torch.Size([768, 2048])\n",
      "Number of parameters: 1572864\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.0.linear2.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.0.norm1.weight\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.0.norm1.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.0.norm2.weight\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.0.norm2.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.1.self_attn.in_proj_weight\n",
      "Shape: torch.Size([2304, 768])\n",
      "Number of parameters: 1769472\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.1.self_attn.in_proj_bias\n",
      "Shape: torch.Size([2304])\n",
      "Number of parameters: 2304\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.1.self_attn.out_proj.weight\n",
      "Shape: torch.Size([768, 768])\n",
      "Number of parameters: 589824\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.1.self_attn.out_proj.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.1.linear1.weight\n",
      "Shape: torch.Size([2048, 768])\n",
      "Number of parameters: 1572864\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.1.linear1.bias\n",
      "Shape: torch.Size([2048])\n",
      "Number of parameters: 2048\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.1.linear2.weight\n",
      "Shape: torch.Size([768, 2048])\n",
      "Number of parameters: 1572864\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.1.linear2.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.1.norm1.weight\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.1.norm1.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.1.norm2.weight\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.1.norm2.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.2.self_attn.in_proj_weight\n",
      "Shape: torch.Size([2304, 768])\n",
      "Number of parameters: 1769472\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.2.self_attn.in_proj_bias\n",
      "Shape: torch.Size([2304])\n",
      "Number of parameters: 2304\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.2.self_attn.out_proj.weight\n",
      "Shape: torch.Size([768, 768])\n",
      "Number of parameters: 589824\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.2.self_attn.out_proj.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.2.linear1.weight\n",
      "Shape: torch.Size([2048, 768])\n",
      "Number of parameters: 1572864\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.2.linear1.bias\n",
      "Shape: torch.Size([2048])\n",
      "Number of parameters: 2048\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.2.linear2.weight\n",
      "Shape: torch.Size([768, 2048])\n",
      "Number of parameters: 1572864\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.2.linear2.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.2.norm1.weight\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.2.norm1.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.2.norm2.weight\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.2.norm2.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.3.self_attn.in_proj_weight\n",
      "Shape: torch.Size([2304, 768])\n",
      "Number of parameters: 1769472\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.3.self_attn.in_proj_bias\n",
      "Shape: torch.Size([2304])\n",
      "Number of parameters: 2304\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.3.self_attn.out_proj.weight\n",
      "Shape: torch.Size([768, 768])\n",
      "Number of parameters: 589824\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.3.self_attn.out_proj.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.3.linear1.weight\n",
      "Shape: torch.Size([2048, 768])\n",
      "Number of parameters: 1572864\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.3.linear1.bias\n",
      "Shape: torch.Size([2048])\n",
      "Number of parameters: 2048\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.3.linear2.weight\n",
      "Shape: torch.Size([768, 2048])\n",
      "Number of parameters: 1572864\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.3.linear2.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.3.norm1.weight\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.3.norm1.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.3.norm2.weight\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.3.norm2.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.4.self_attn.in_proj_weight\n",
      "Shape: torch.Size([2304, 768])\n",
      "Number of parameters: 1769472\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.4.self_attn.in_proj_bias\n",
      "Shape: torch.Size([2304])\n",
      "Number of parameters: 2304\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.4.self_attn.out_proj.weight\n",
      "Shape: torch.Size([768, 768])\n",
      "Number of parameters: 589824\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.4.self_attn.out_proj.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.4.linear1.weight\n",
      "Shape: torch.Size([2048, 768])\n",
      "Number of parameters: 1572864\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.4.linear1.bias\n",
      "Shape: torch.Size([2048])\n",
      "Number of parameters: 2048\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.4.linear2.weight\n",
      "Shape: torch.Size([768, 2048])\n",
      "Number of parameters: 1572864\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.4.linear2.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.4.norm1.weight\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.4.norm1.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.4.norm2.weight\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.layers.4.norm2.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.norm.weight\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: transformer.transformer.norm.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: policy_feedforward.0.weight\n",
      "Shape: torch.Size([768, 768])\n",
      "Number of parameters: 589824\n",
      "------------------------------\n",
      "Layer: policy_feedforward.0.bias\n",
      "Shape: torch.Size([768])\n",
      "Number of parameters: 768\n",
      "------------------------------\n",
      "Layer: policy_feedforward.2.weight\n",
      "Shape: torch.Size([50257, 768])\n",
      "Number of parameters: 38597376\n",
      "------------------------------\n",
      "Layer: policy_feedforward.2.bias\n",
      "Shape: torch.Size([50257])\n",
      "Number of parameters: 50257\n",
      "------------------------------\n",
      "Layer: value_feedforward.0.weight\n",
      "Shape: torch.Size([2048, 768])\n",
      "Number of parameters: 1572864\n",
      "------------------------------\n",
      "Layer: value_feedforward.0.bias\n",
      "Shape: torch.Size([2048])\n",
      "Number of parameters: 2048\n",
      "------------------------------\n",
      "Layer: value_feedforward.2.weight\n",
      "Shape: torch.Size([1, 2048])\n",
      "Number of parameters: 2048\n",
      "------------------------------\n",
      "Layer: value_feedforward.2.bias\n",
      "Shape: torch.Size([1])\n",
      "Number of parameters: 1\n",
      "------------------------------\n",
      "Layer: token_embedding.weight\n",
      "Shape: torch.Size([50257, 768])\n",
      "Number of parameters: 38597376\n",
      "------------------------------\n",
      "==================================================\n",
      "Total trainable parameters: 112498770\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def print_model_parameters(model):\n",
    "    \"\"\"\n",
    "    Print all parameters of a PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "    model (nn.Module): The PyTorch model to analyze\n",
    "    \n",
    "    Returns:\n",
    "    None: Prints information to stdout\n",
    "    \"\"\"\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Layer: {name}\")\n",
    "            print(f\"Shape: {param.shape}\")\n",
    "            print(f\"Number of parameters: {param.numel()}\")\n",
    "            print(\"-\" * 30)\n",
    "            total_params += param.numel()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total trainable parameters: {total_params}\")\n",
    "vocab_size = 50257  # GPT-2 tokenizer vocabulary size\n",
    "d_embed = 768  # Embedding dimension\n",
    "epochs = 30\n",
    "max_sequence_length = 1000\n",
    "chunk_size = 200  # New sequence chunk size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = CausalModel(vocab_size, max_sequence_length=chunk_size, num_layers=12, feed_forward_dim=d_embed*4, dropout=0.15).to(device)\n",
    "thoughtsformer = ThoughtsFormer(thought_length=0, vocab_size= vocab_size, max_sequence_length=chunk_size, verbose=False, num_layers=5, dropout=0.15).to(device)\n",
    "\n",
    "# print_model_parameters(model)\n",
    "# print()\n",
    "# print()\n",
    "# print()\n",
    "# print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "# print()\n",
    "# print()\n",
    "# print()\n",
    "print_model_parameters(thoughtsformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "n_thoughts_taken (1) > thought_len (0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\Documents\\GitHub\\NextTextPredictionNormal\\one_step_thoughtsformer.py:254\u001b[0m, in \u001b[0;36mThoughtsFormer.forward_ppo\u001b[1;34m(self, state_embeddings, padding_mask, n_thoughts_taken)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_ppo\u001b[39m(\u001b[38;5;28mself\u001b[39m, state_embeddings: torch\u001b[38;5;241m.\u001b[39mTensor, padding_mask: torch\u001b[38;5;241m.\u001b[39mTensor, n_thoughts_taken: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    251\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m  Takes in the current state and the padding mask, outputs the logits for the next action and value for every token in the sequence\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m  '''\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m n_thoughts_taken \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthought_length, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_thoughts_taken (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_thoughts_taken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) > thought_len (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthought_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m state_embeddings\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_embeddings must be three dimensional, where the dimensons are (batch_size, seq_len, ndim). Instead recieved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_embeddings\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensional input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m state_embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_embed, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms final dimension must be of size d_embed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_embed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), instead recieved size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: n_thoughts_taken (1) > thought_len (0)"
     ]
    }
   ],
   "source": [
    "x.forward_ppo(test_embeds,padding_mask,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.0476, -0.2576, -0.1962,  ..., -0.2677,  0.0543,  0.1103],\n",
      "         [ 0.1308, -0.2663, -0.0288,  ..., -0.1060,  0.1292, -0.0711],\n",
      "         [ 0.1284, -0.2976,  0.0113,  ..., -0.1118,  0.1556, -0.0875],\n",
      "         ...,\n",
      "         [ 0.0691, -0.2967,  0.0103,  ..., -0.1713,  0.1719, -0.0755],\n",
      "         [ 0.1073, -0.2999,  0.0263,  ..., -0.1239,  0.1640, -0.0897],\n",
      "         [ 0.1204, -0.3086,  0.0207,  ..., -0.1234,  0.1673, -0.0875]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[[ 0.0584],\n",
      "         [ 0.0772],\n",
      "         [ 0.0653],\n",
      "         [ 0.0506],\n",
      "         [ 0.0171],\n",
      "         [-0.0348],\n",
      "         [-0.0538],\n",
      "         [ 0.0043],\n",
      "         [ 0.0483],\n",
      "         [ 0.0472]]], grad_fn=<ViewBackward0>))\n"
     ]
    }
   ],
   "source": [
    "print(x.forward_ppo(test_embeds,padding_mask,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original embeddings \n",
      " tensor([[[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]]])\n",
      "embeddings right before positional encodings tensor([[[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]]])\n",
      "embeddings right before transformer forward tensor([[[ 0.1834,  2.8287,  0.7036,  3.2256, -0.0432,  2.6525],\n",
      "         [ 1.0249,  2.3690,  0.7500,  3.2246, -0.0411,  2.6525],\n",
      "         [ 1.0927,  1.4126,  0.7963,  3.2213, -0.0389,  2.6525],\n",
      "         [ 0.3246,  0.8388,  0.8424,  3.2159, -0.0368,  2.6525],\n",
      "         [-0.5734,  1.1751,  0.8882,  3.2084, -0.0346,  2.6525],\n",
      "         [-0.7755,  2.1124,  0.9336,  3.1988, -0.0324,  2.6525],\n",
      "         [-0.0960,  2.7889,  0.9785,  3.1871, -0.0303,  2.6525],\n",
      "         [ 0.8404,  2.5826,  1.0229,  3.1733, -0.0281,  2.6524],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<ConstantPadNdBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings right after transformer forward tensor([[[-0.4502,  0.0157, -0.7128,  1.0435, -1.3964,  1.5000],\n",
      "         [ 0.0454, -0.4041, -0.6575,  1.0880, -1.4878,  1.4160],\n",
      "         [ 0.2126, -0.9419, -0.5434,  1.2524, -1.2555,  1.2758],\n",
      "         [-0.0529, -1.1244, -0.4663,  1.4234, -1.0001,  1.2203],\n",
      "         [-0.5030, -0.8702, -0.4714,  1.5033, -0.9361,  1.2772],\n",
      "         [-0.7859, -0.3599, -0.5639,  1.4286, -1.0540,  1.3352],\n",
      "         [-0.6008, -0.0656, -0.6753,  1.2985, -1.2997,  1.3429],\n",
      "         [-0.0594, -0.3293, -0.6952,  1.2407, -1.4663,  1.3096],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "future embeddings\n",
      " tensor([[[-0.4502,  0.0157, -0.7128,  1.0435, -1.3964,  1.5000],\n",
      "         [ 0.0454, -0.4041, -0.6575,  1.0880, -1.4878,  1.4160],\n",
      "         [ 0.2126, -0.9419, -0.5434,  1.2524, -1.2555,  1.2758],\n",
      "         [-0.0529, -1.1244, -0.4663,  1.4234, -1.0001,  1.2203],\n",
      "         [-0.5030, -0.8702, -0.4714,  1.5033, -0.9361,  1.2772],\n",
      "         [-0.7859, -0.3599, -0.5639,  1.4286, -1.0540,  1.3352],\n",
      "         [-0.6008, -0.0656, -0.6753,  1.2985, -1.2997,  1.3429],\n",
      "         [-0.0594, -0.3293, -0.6952,  1.2407, -1.4663,  1.3096],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = 20\n",
    "d_embed = 6\n",
    "n_extra_thoughts = 1 #\n",
    "\n",
    "y = ThoughtsFormer(thought_length=n_extra_thoughts, vocab_size=5, max_context_length=context_length, num_layers=1, d_embed=d_embed,n_head=1, verbose=True,dropout=0)\n",
    "\n",
    "\n",
    "token_count = 8\n",
    "real_token_count = token_count\n",
    "\n",
    "tokens = torch.cat((torch.ones(1,token_count,d_embed), torch.zeros(1,context_length - token_count, d_embed)), dim=1)\n",
    "padding_arr = torch.cat((torch.zeros(1,token_count),torch.ones(1,context_length - token_count)), dim=-1)\n",
    "\n",
    "y_embeddings = y.forward_ppo(tokens, padding_arr,0)\n",
    "# get_logits_for_sequences(token_count, y_embeddings, n_extra_thoughts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1624, -0.0141, -1.0231,  0.3266,  0.6928],\n",
       "         [ 1.1951, -0.0971, -0.9133,  0.0833,  0.7577],\n",
       "         [ 1.2024, -0.1614, -0.7602, -0.1139,  0.7183],\n",
       "         [ 1.1957, -0.1226, -0.7297, -0.0457,  0.5896],\n",
       "         [ 1.2037, -0.0079, -0.8705,  0.2255,  0.4994],\n",
       "         [ 1.2063,  0.0725, -1.0326,  0.4331,  0.5230],\n",
       "         [ 1.2148,  0.0616, -1.0846,  0.4058,  0.6260],\n",
       "         [ 1.2395, -0.0192, -1.0031,  0.1738,  0.7255]]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "\n",
    "\n",
    "class env():\n",
    "    def __init__(self,  token_sequence, thought_length):\n",
    "        vocab_size = 30000\n",
    "        self.thoughtsformer = ThoughtsFormer(thought_length=thought_length, vocab_size=vocab_size, max_context_length=10, num_layers=5)\n",
    "        \n",
    "        self.x, self.y = self.extract_labels_and_targets_from(token_sequence)\n",
    "        \n",
    "        self.thought_length = thought_length\n",
    "        self.thoughts_taken = 0\n",
    "    \n",
    "    def extract_labels_and_targets_from(self, token_sequence):\n",
    "\n",
    "        inputs = self.tokenizer(token_sequence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.gpt2_model(**inputs)\n",
    "        \n",
    "        x = outputs.last_hidden_state.squeeze(0)[:-1]  # All but last token\n",
    "        y = inputs.input_ids.squeeze()[1:]  # All but first token ID\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def reset(self):\n",
    "        self.thoughts_taken = 0\n",
    "    def step(self, action_embeddings, state_embeddings):\n",
    "        self.thoughts_taken += 1\n",
    "        next_embeddings = action_embeddings\n",
    "        if self.thoughts_taken != self.thought_length:\n",
    "            next_state_embeddings = self.thoughtsformer.insert_thoughts(next_embeddings, state_embeddings, self.thoughts_taken + 1)\n",
    "\n",
    "        return self.reward(next_state_embeddings, self.thoughts_taken)\n",
    "    \n",
    "    def reward(self, next_state_embeddings, thoughts_taken):\n",
    "        \n",
    "        \n",
    "        if thoughts_taken == self.thought_length:\n",
    "            logits = self.thoughtsformer.get_logits_for_sequences(next_state_embeddings, thoughts_taken)\n",
    "            loss_fn(logits, targets=self.y)\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megatron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
