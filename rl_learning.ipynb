{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from gymnasium import Env, spaces\n",
    "from tiny_shakespeare import TinyShakespeareDataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from token_level_ppo import ThoughtsFormerPolicy, TokenLevelPPO\n",
    "\n",
    "\n",
    "def token_batched_reshape_with_offset(x: torch.Tensor, max_seq_length: int, thoughts_taken: int) -> torch.Tensor:\n",
    "    thoughts = thoughts_taken + 1\n",
    "    max_thoughts = x.size(1) // max_seq_length\n",
    "    x = x[:,:max_seq_length*thoughts].view(x.size(0), max_seq_length, thoughts)\n",
    "    return F.pad(x,(0, (max_thoughts - thoughts)))\n",
    "        \n",
    "class ThoughtsFormerEnv(Env):\n",
    "    def __init__(self, vocab_size, max_sequence_length, max_thought_length):\n",
    "        super(ThoughtsFormerEnv, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_sequence_length = max_sequence_length \n",
    "        self.max_thought_length = max_thought_length\n",
    "        self.max_context_length = max_sequence_length * (max_thought_length+1)\n",
    "        \n",
    "        # Logits\n",
    "        self.action_space = spaces.Box(low=-1000, high=1000, shape=(max_sequence_length,vocab_size), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"state\" : spaces.MultiDiscrete([vocab_size] * self.max_context_length),\n",
    "            \"thought_step\" : spaces.Discrete(max_thought_length+1)\n",
    "        })\n",
    "        \n",
    "        self.dataset = TinyShakespeareDataset(max_sequence_length,window_offset=max_sequence_length//4)\n",
    "        self.dataset_len = len(self.dataset)\n",
    "        self.dataset_iter = 0\n",
    "        \n",
    "        self.thought_step = 0\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)  # Ensures Gymnasium's seeding is properly handled\n",
    "        \n",
    "        self.state, self.labels = self.dataset[self.dataset_iter]\n",
    "        self.state = F.pad(self.state, (0,self.max_context_length-self.max_sequence_length))\n",
    "     \n",
    "        # prepare self.state and massively elongate\n",
    "        self.dataset_iter += 1\n",
    "        if self.dataset == self.dataset_len:\n",
    "            self.dataset_iter = 0\n",
    "            self.dataset.shuffle()\n",
    "\n",
    "        self.thought_step = 0\n",
    "        \n",
    "        obs = {\n",
    "            'state' : self.state.numpy(),\n",
    "            'thought_step' : self.thought_step\n",
    "        }\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state = self.state.view(1,-1)\n",
    "        \n",
    "        probs = F.softmax(torch.from_numpy(action), dim = -1)\n",
    "        sampled_tokens = torch.multinomial(\n",
    "            probs.view(-1, probs.size(-1)),\n",
    "            num_samples=1\n",
    "        ).view(-1, probs.size(0))\n",
    "            \n",
    "        \n",
    "        if self.thought_step == self.max_thought_length:\n",
    "            reward = self.reward(action)\n",
    "            # print(reward.shape)\n",
    "            done = True\n",
    "        else:\n",
    "            reward = torch.zeros(self.max_sequence_length)\n",
    "            done = False\n",
    "            # Add the thought!\n",
    "            self.state = token_batched_reshape_with_offset(self.state, self.max_sequence_length, self.thought_step) # (batch x max_seq_len, (max_thought_len+1))\n",
    "            # before\n",
    "            # plt.imshow(self.state[0,:10]); plt.show()\n",
    "            \n",
    "            self.state[:,:,self.thought_step+1] = sampled_tokens\n",
    "            # plt.imshow(self.state[0,:10]); plt.show()\n",
    "            self.state =  self.state.view(1,-1)\n",
    "        \n",
    "        self.state = self.state.view(-1)\n",
    "        \n",
    "        self.thought_step += 1 # Add it first so when the model looks at s_t+1 it thinks with another thought\n",
    "        obs = {\n",
    "            'state' : self.state.numpy(),\n",
    "            'thought_step' : self.thought_step\n",
    "        }\n",
    "        \n",
    "        info = {'reward' : reward, 'actions_taken' : sampled_tokens.squeeze(0)} \n",
    "        \n",
    "        # print(info)\n",
    "        \n",
    "        return obs, -np.inf, done, False, info #obs, reward, done, truncated, info\n",
    "\n",
    "    def reward(self, action):\n",
    "        return -F.cross_entropy(torch.tensor(action), self.labels, reduction='none')\n",
    "    \n",
    "torch.manual_seed(0)\n",
    "\n",
    "env = ThoughtsFormerEnv(vocab_size=50257, max_sequence_length=512,max_thought_length=1)\n",
    "\n",
    "ppo = TokenLevelPPO(ThoughtsFormerPolicy, env, n_steps=2, batch_size=2, max_sequence_length=512, verbose=2)\n",
    "\n",
    "ppo.learn(1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m    \n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\Documents\\GitHub\\Thoughtsformer\\token_level_ppo.py:352\u001b[0m, in \u001b[0;36mTokenLevelPPO.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    350\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(log_probs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 352\u001b[0m new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([infos[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(infos))])\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    354\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([infos[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions_taken\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(infos))])\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[1;32mIn[1], line 66\u001b[0m, in \u001b[0;36mThoughtsFormerEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     65\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(action), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m sampled_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, probs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthought_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_thought_length:\n\u001b[0;32m     73\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(action)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "x = TinyShakespeareDataset(512,1024*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {'state' : x[0][0].view(1,-1).to('cuda'), 'thought_step' : 1}\n",
    "a,b = ppo.policy.forward(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0055, 0.0048, 0.0055,  ..., 0.0021, 0.0021, 0.0021],\n",
       "         [0.0047, 0.0052, 0.0047,  ..., 0.0024, 0.0024, 0.0024],\n",
       "         [0.0053, 0.0051, 0.0053,  ..., 0.0027, 0.0027, 0.0027],\n",
       "         ...,\n",
       "         [0.0104, 0.0040, 0.0104,  ..., 0.0233, 0.0233, 0.0233],\n",
       "         [0.0106, 0.0039, 0.0106,  ..., 0.0228, 0.0228, 0.0228],\n",
       "         [0.0110, 0.0038, 0.0110,  ..., 0.0236, 0.0236, 0.0236]]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = F.softmax(a,dim=-1)\n",
    "n[:,:,n.argmax(dim=-1).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "p = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.name_to_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ThoughtsFormerEnv.__init__() missing 3 required positional arguments: 'vocab_size', 'max_sequence_length', and 'max_thought_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_value_from_obs\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(obs)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 20\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mThoughtsFormerEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: ThoughtsFormerEnv.__init__() missing 3 required positional arguments: 'vocab_size', 'max_sequence_length', and 'max_thought_length'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from thoughtsformer import ThoughtsFormer, simple_batched_reshape_with_offset\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "class _ThoughtsFormerPolicy(ActorCriticPolicy):\n",
    "    def __init__(self):\n",
    "        self.model = ThoughtsFormer.from_pretrained_GPT2()\n",
    "        \n",
    "    def forward(self, obs: dict):\n",
    "        assert 'state' in obs and 'thought_step' in obs, f\"'state' and 'thought_step' should be keys of the observation dictionary\"\n",
    "        return self.model.forward_ppo_with_tokens(obs['state'], torch.zeros_like(obs['state']), obs['thought_step'])\n",
    "    \n",
    "    def _get_action_dist_from_obs(self, obs):\n",
    "        return self.forward(obs)[0]\n",
    "\n",
    "    def _get_value_from_obs(self, obs):\n",
    "        return self.forward(obs)[1]\n",
    "    \n",
    "env = ThoughtsFormerEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state torch.Size([11264])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TokenLevelPPO.collect_rollouts() got an unexpected keyword argument 'n_rollout_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: TokenLevelPPO.collect_rollouts() got an unexpected keyword argument 'n_rollout_steps'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the environment to ensure compatibility\n",
    "from token_level_ppo import TokenLevelPPO, TokenLevelRolloutBuffer, ThoughtsFormerPolicy\n",
    "ppo = TokenLevelPPO(ThoughtsFormerPolicy, env, n_steps=2, batch_size=2, max_sequence_length=512, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 0, 0],\n",
       "         [3, 4, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "x = torch.tensor([1,2,3,4,0,0,0,0,0,0,0,0,0,0,0,0]).view(1,-1)\n",
    "\n",
    "\n",
    "def simple_batched_reshape_with_offset(x: torch.Tensor, max_seq_length: int, thoughts_taken: int) -> torch.Tensor:\n",
    "    thoughts = thoughts_taken + 1\n",
    "    max_thoughts = x.size(1) // max_seq_length\n",
    "    x = x[:,:max_seq_length*thoughts].view(x.size(0), max_seq_length, thoughts)\n",
    "    return F.pad(x,(0, (max_thoughts - thoughts)))\n",
    "\n",
    "simple_batched_reshape_with_offset(x,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.4     |\n",
      "|    ep_rew_mean     | -0.292   |\n",
      "| time/              |          |\n",
      "|    fps             | 327      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 32       |\n",
      "|    ep_rew_mean     | 0.524    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.4     |\n",
      "|    ep_rew_mean     | 0.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [2 2], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [2 0], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.9     |\n",
      "|    ep_rew_mean     | 0.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [2 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.5     |\n",
      "|    ep_rew_mean     | 0.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [2 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [2 0], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.91     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [2 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.79     |\n",
      "|    ep_rew_mean     | 0.98     |\n",
      "| time/              |          |\n",
      "|    fps             | 355      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [2 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.74     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 353      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.92     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 349      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.14     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from gymnasium import Env, spaces\n",
    "\n",
    "class CustomEnv(Env):\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(4)  # 4 actions (up, right, down, left)\n",
    "        self.observation_space = spaces.MultiDiscrete([3, 4])  # (y, x) coordinates\n",
    "        self.state = (2, 0)  # Initial state\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)  # Ensures Gymnasium's seeding is properly handled\n",
    "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
    "        self.state = (2, 0)\n",
    "        return np.array(self.state), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action in the environment.\"\"\"\n",
    "        y, x = self.state\n",
    "\n",
    "        if action == 0:  # up\n",
    "            y += 1\n",
    "        elif action == 1:  # right\n",
    "            x += 1\n",
    "        elif action == 2:  # down\n",
    "            y -= 1\n",
    "        elif action == 3:  # left\n",
    "            x -= 1\n",
    "\n",
    "        # Keep coordinates within bounds\n",
    "        y, x = max(0, min(2, y)), max(0, min(3, x))\n",
    "        self.state = (y, x)\n",
    "\n",
    "        # Compute reward and done\n",
    "        reward, done = self.reward(self.state\n",
    "                                   )\n",
    "        return np.array(self.state), reward, done, False, {}\n",
    "\n",
    "    def reward(self, state):\n",
    "        if state == (0, 3):\n",
    "            return 1, True  # Goal state with reward\n",
    "        elif state == (1, 3):\n",
    "            return -1, True  # Failure state with negative reward\n",
    "        else:\n",
    "            return 0, False  # No reward, episode continues\n",
    "\n",
    "# Check the environment to ensure compatibility\n",
    "env = CustomEnv()\n",
    "check_env(env)\n",
    "\n",
    "# Create the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "# Test the trained model\n",
    "obs = env.reset()[0]\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    print(f\"State: {obs}, Reward: {reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = np.zeros((3,4))\n",
    "policy = np.zeros((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "class Actions(Enum):\n",
    "    up = 0,\n",
    "    right = 1,\n",
    "    down = 2,\n",
    "    left  = 3,\n",
    "    none = -1\n",
    "\n",
    "class env():\n",
    "    \n",
    "    actions = (Actions.up, Actions.right, Actions.down, Actions.left)\n",
    "    states = (\n",
    "            ((0,0), (0,1), (0,2), (0,3)),\n",
    "            ((1,0), (1,1), (1,2), (1,3)),\n",
    "            ((2,0), (2,1), (2,2), (2,3))\n",
    "    )\n",
    "    \n",
    "    iter_states = (\n",
    "            (0,0), (0,1), (0,2), (0,3),\n",
    "            (1,0), (1,1), (1,2), (1,3),\n",
    "            (2,0), (2,1), (2,2), (2,3)\n",
    "    )\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def step(self, state, action):\n",
    "        y, x = state\n",
    "        \n",
    "        if action == Actions.up:\n",
    "            y += 1\n",
    "        if action == Actions.right:\n",
    "            x += 1\n",
    "        if action == Actions.down:\n",
    "            y -= 1\n",
    "        if action == Actions.left:\n",
    "            x -= 1\n",
    "\n",
    "        y, x = max(y,0), max(x,0)\n",
    "        y, x = min(y,2), min(x,3)\n",
    "        state_next = (y,x)\n",
    "        reward, done = self.reward((y,x))\n",
    "        return state_next, reward, done\n",
    "        \n",
    "    def reward(self, next_state):\n",
    "        if next_state == (0,3):\n",
    "            return 1, True\n",
    "        elif next_state == (1,3):\n",
    "            return -1, True\n",
    "        else:\n",
    "            return 0, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 3\n",
    "gamma = 0.9\n",
    "e = env()\n",
    "for i in range(H):\n",
    "    for state in env.iter_states:\n",
    "        \n",
    "        if state == (0, 3) or state == (1, 3):\n",
    "            continue\n",
    "        \n",
    "        if state == (1, 1):\n",
    "            continue\n",
    "        \n",
    "        max_reward = 0\n",
    "        p = Actions.none.value\n",
    "        for idx, action in enumerate(env.actions):\n",
    "            n_state, rew = e.step(state, action)\n",
    "            n = rew + gamma * value[n_state]\n",
    "            if n > max_reward:\n",
    "                max_reward = n\n",
    "                p = action.value[0]\n",
    "        policy[state] = p\n",
    "        value[state] = max_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81  , 0.9   , 1.    , 0.    ],\n",
       "       [0.729 , 0.    , 0.9   , 0.    ],\n",
       "       [0.6561, 0.729 , 0.81  , 0.729 ]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 0.],\n",
       "       [2., 0., 2., 0.],\n",
       "       [1., 1., 2., 3.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy\n",
    "\n",
    "class Actions(Enum):\n",
    "    down = 0,\n",
    "    right = 1,\n",
    "    up = 2,\n",
    "    left  = 3,\n",
    "    none = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megatron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
