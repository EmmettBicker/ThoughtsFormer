{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from gymnasium import Env, spaces\n",
    "from tiny_shakespeare import TinyShakespeareDataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from token_level_ppo import ThoughtsFormerPolicy, TokenLevelPPO\n",
    "\n",
    "\n",
    "def token_batched_reshape_with_offset(x: torch.Tensor, max_seq_length: int, thoughts_taken: int) -> torch.Tensor:\n",
    "    thoughts = thoughts_taken + 1\n",
    "    max_thoughts = x.size(1) // max_seq_length\n",
    "    x = x[:,:max_seq_length*thoughts].view(x.size(0), max_seq_length, thoughts)\n",
    "    return F.pad(x,(0, (max_thoughts - thoughts)))\n",
    "        \n",
    "class ThoughtsFormerEnv(Env):\n",
    "    def __init__(self, vocab_size, max_sequence_length, max_thought_length):\n",
    "        super(ThoughtsFormerEnv, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_sequence_length = max_sequence_length \n",
    "        self.max_thought_length = max_thought_length\n",
    "        self.max_context_length = max_sequence_length * (max_thought_length+1)\n",
    "        \n",
    "        # Logits\n",
    "        self.action_space = spaces.Box(low=-100, high=100, shape=(max_sequence_length,vocab_size), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"state\" : spaces.MultiDiscrete([vocab_size] * self.max_context_length),\n",
    "            \"thought_step\" : spaces.Discrete(max_thought_length+1)\n",
    "        })\n",
    "        \n",
    "        self.dataset = TinyShakespeareDataset(max_sequence_length,window_offset=max_sequence_length//4)\n",
    "        self.dataset_len = len(self.dataset)\n",
    "        self.dataset_iter = 0\n",
    "        \n",
    "        self.thought_step = 0\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)  # Ensures Gymnasium's seeding is properly handled\n",
    "        \n",
    "        self.state, self.labels = self.dataset[self.dataset_iter]\n",
    "        self.state = F.pad(self.state, (0,self.max_context_length-self.max_sequence_length))\n",
    "     \n",
    "        # prepare self.state and massively elongate\n",
    "        self.dataset_iter += 1\n",
    "        if self.dataset == self.dataset_len:\n",
    "            self.dataset_iter = 0\n",
    "\n",
    "        self.thought_step = 0\n",
    "        \n",
    "        obs = {\n",
    "            'state' : self.state.numpy(),\n",
    "            'thought_step' : self.thought_step\n",
    "        }\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state = self.state.view(1,-1)\n",
    "        \n",
    "        probs = F.softmax(torch.from_numpy(action), dim = -1)\n",
    "        sampled_tokens = torch.multinomial(\n",
    "            probs.view(-1, probs.size(-1)),\n",
    "            num_samples=1\n",
    "        ).view(-1, probs.size(0))\n",
    "            \n",
    "        \n",
    "        if self.thought_step == self.max_thought_length:\n",
    "            reward = self.reward(action)\n",
    "            # print(reward.shape)\n",
    "            done = True\n",
    "        else:\n",
    "            reward = torch.zeros(self.max_sequence_length)\n",
    "            done = False\n",
    "            # Add the thought!\n",
    "            self.state = token_batched_reshape_with_offset(self.state, self.max_sequence_length, self.thought_step) # (batch x max_seq_len, (max_thought_len+1))\n",
    "            # before\n",
    "            # plt.imshow(self.state[0,:10]); plt.show()\n",
    "            \n",
    "            self.state[:,:,self.thought_step+1] = sampled_tokens\n",
    "            # plt.imshow(self.state[0,:10]); plt.show()\n",
    "            self.state =  self.state.view(1,-1)\n",
    "        \n",
    "        self.state = self.state.view(-1)\n",
    "        obs = {\n",
    "            'state' : self.state.numpy(),\n",
    "            'thought_step' : self.thought_step\n",
    "        }\n",
    "        \n",
    "        info = {'reward' : reward, 'actions_taken' : sampled_tokens.squeeze(0)} \n",
    "        \n",
    "        # print(info)\n",
    "        self.thought_step += 1\n",
    "        return obs, -np.inf, done, False, info #obs, reward, done, truncated, info\n",
    "\n",
    "    def reward(self, action):\n",
    "        return -F.cross_entropy(torch.tensor(action), self.labels, reduction='none')\n",
    "    \n",
    "\n",
    "env = ThoughtsFormerEnv(vocab_size=50257, max_sequence_length=512,max_thought_length=1)\n",
    "\n",
    "# ppo = TokenLevelPPO(ThoughtsFormerPolicy, env, n_steps=4, batch_size=2, max_sequence_length=512, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "ppo = TokenLevelPPO.load(max_sequence_length=512, path=\"ppo_thoughtsformer2\", env=env, n_steps=4,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m    \n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\Documents\\GitHub\\Thoughtsformer\\token_level_ppo.py:336\u001b[0m, in \u001b[0;36mTokenLevelPPO.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    335\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 336\u001b[0m     log_probs, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m log_probs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\Documents\\GitHub\\Thoughtsformer\\token_level_ppo.py:36\u001b[0m, in \u001b[0;36mThoughtsFormerPolicy.forward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m obs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthought_step\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m obs, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthought_step\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be keys of the observation dictionary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 36\u001b[0m     logits, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_ppo_with_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthought_step\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits, values\n",
      "File \u001b[1;32mc:\\Users\\bicke\\Documents\\GitHub\\Thoughtsformer\\thoughtsformer.py:391\u001b[0m, in \u001b[0;36mThoughtsFormer.forward_ppo_with_tokens\u001b[1;34m(self, tokens, padding_mask, n_thoughts_taken)\u001b[0m\n\u001b[0;32m    388\u001b[0m state_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding(tokens)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# Call the existing forward_ppo method\u001b[39;00m\n\u001b[1;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_thoughts_taken\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\Documents\\GitHub\\Thoughtsformer\\thoughtsformer.py:366\u001b[0m, in \u001b[0;36mThoughtsFormer.forward_ppo\u001b[1;34m(self, state_embeddings, padding_mask, n_thoughts_taken)\u001b[0m\n\u001b[0;32m    363\u001b[0m debug_print(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuture embeddings\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, next_embeddings,  flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug)\n\u001b[0;32m    364\u001b[0m action_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tokens_at_action_location(next_embeddings, n_thoughts_taken)\n\u001b[1;32m--> 366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_feedforward\u001b[49m(action_embeddings), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_feedforward(action_embeddings)\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1716\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[1;32m-> 1716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1718\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_trace_dispatch_regular.py:326\u001b[0m, in \u001b[0;36mThreadTracer.__call__\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# ENDIF\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, frame, event, arg):\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m''' This is the callback used when we enter some context in the debugger.\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m        We also decorate the thread we are in with info about the debugging.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m            This is the global debugger (this method should actually be added as a method to it).\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m        '''\u001b[39;00m\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;66;03m# IFDEF CYTHON\u001b[39;00m\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;66;03m# cdef str filename;\u001b[39;00m\n\u001b[0;32m    341\u001b[0m         \u001b[38;5;66;03m# cdef str base;\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[38;5;66;03m# DEBUG = 'code_to_debug' in frame.f_code.co_filename\u001b[39;00m\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;66;03m# if DEBUG: print('ENTER: trace_dispatch: %s %s %s %s' % (frame.f_code.co_filename, frame.f_lineno, event, frame.f_code.co_name))\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppo.learn(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "x = TinyShakespeareDataset(512,1024*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {'state' : x[0][0].view(1,-1).to('cuda'), 'thought_step' : 1}\n",
    "a,b = ppo.policy.forward(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0055, 0.0048, 0.0055,  ..., 0.0021, 0.0021, 0.0021],\n",
       "         [0.0047, 0.0052, 0.0047,  ..., 0.0024, 0.0024, 0.0024],\n",
       "         [0.0053, 0.0051, 0.0053,  ..., 0.0027, 0.0027, 0.0027],\n",
       "         ...,\n",
       "         [0.0104, 0.0040, 0.0104,  ..., 0.0233, 0.0233, 0.0233],\n",
       "         [0.0106, 0.0039, 0.0106,  ..., 0.0228, 0.0228, 0.0228],\n",
       "         [0.0110, 0.0038, 0.0110,  ..., 0.0236, 0.0236, 0.0236]]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = F.softmax(a,dim=-1)\n",
    "n[:,:,n.argmax(dim=-1).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "p = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.name_to_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ThoughtsFormerEnv.__init__() missing 3 required positional arguments: 'vocab_size', 'max_sequence_length', and 'max_thought_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_value_from_obs\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(obs)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 20\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mThoughtsFormerEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: ThoughtsFormerEnv.__init__() missing 3 required positional arguments: 'vocab_size', 'max_sequence_length', and 'max_thought_length'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from thoughtsformer import ThoughtsFormer, simple_batched_reshape_with_offset\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "class _ThoughtsFormerPolicy(ActorCriticPolicy):\n",
    "    def __init__(self):\n",
    "        self.model = ThoughtsFormer.from_pretrained_GPT2()\n",
    "        \n",
    "    def forward(self, obs: dict):\n",
    "        assert 'state' in obs and 'thought_step' in obs, f\"'state' and 'thought_step' should be keys of the observation dictionary\"\n",
    "        return self.model.forward_ppo_with_tokens(obs['state'], torch.zeros_like(obs['state']), obs['thought_step'])\n",
    "    \n",
    "    def _get_action_dist_from_obs(self, obs):\n",
    "        return self.forward(obs)[0]\n",
    "\n",
    "    def _get_value_from_obs(self, obs):\n",
    "        return self.forward(obs)[1]\n",
    "    \n",
    "env = ThoughtsFormerEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state torch.Size([11264])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TokenLevelPPO.collect_rollouts() got an unexpected keyword argument 'n_rollout_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: TokenLevelPPO.collect_rollouts() got an unexpected keyword argument 'n_rollout_steps'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the environment to ensure compatibility\n",
    "from token_level_ppo import TokenLevelPPO, TokenLevelRolloutBuffer, ThoughtsFormerPolicy\n",
    "ppo = TokenLevelPPO(ThoughtsFormerPolicy, env, n_steps=2, batch_size=2, max_sequence_length=512, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 0, 0],\n",
       "         [3, 4, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "x = torch.tensor([1,2,3,4,0,0,0,0,0,0,0,0,0,0,0,0]).view(1,-1)\n",
    "\n",
    "\n",
    "def simple_batched_reshape_with_offset(x: torch.Tensor, max_seq_length: int, thoughts_taken: int) -> torch.Tensor:\n",
    "    thoughts = thoughts_taken + 1\n",
    "    max_thoughts = x.size(1) // max_seq_length\n",
    "    x = x[:,:max_seq_length*thoughts].view(x.size(0), max_seq_length, thoughts)\n",
    "    return F.pad(x,(0, (max_thoughts - thoughts)))\n",
    "\n",
    "simple_batched_reshape_with_offset(x,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.4     |\n",
      "|    ep_rew_mean     | -0.292   |\n",
      "| time/              |          |\n",
      "|    fps             | 327      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 32       |\n",
      "|    ep_rew_mean     | 0.524    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.4     |\n",
      "|    ep_rew_mean     | 0.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [2 2], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [2 0], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.9     |\n",
      "|    ep_rew_mean     | 0.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [2 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.5     |\n",
      "|    ep_rew_mean     | 0.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [2 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [2 0], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.91     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [2 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.79     |\n",
      "|    ep_rew_mean     | 0.98     |\n",
      "| time/              |          |\n",
      "|    fps             | 355      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [2 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.74     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 353      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.92     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 349      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.14     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from gymnasium import Env, spaces\n",
    "\n",
    "class CustomEnv(Env):\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(4)  # 4 actions (up, right, down, left)\n",
    "        self.observation_space = spaces.MultiDiscrete([3, 4])  # (y, x) coordinates\n",
    "        self.state = (2, 0)  # Initial state\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)  # Ensures Gymnasium's seeding is properly handled\n",
    "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
    "        self.state = (2, 0)\n",
    "        return np.array(self.state), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action in the environment.\"\"\"\n",
    "        y, x = self.state\n",
    "\n",
    "        if action == 0:  # up\n",
    "            y += 1\n",
    "        elif action == 1:  # right\n",
    "            x += 1\n",
    "        elif action == 2:  # down\n",
    "            y -= 1\n",
    "        elif action == 3:  # left\n",
    "            x -= 1\n",
    "\n",
    "        # Keep coordinates within bounds\n",
    "        y, x = max(0, min(2, y)), max(0, min(3, x))\n",
    "        self.state = (y, x)\n",
    "\n",
    "        # Compute reward and done\n",
    "        reward, done = self.reward(self.state\n",
    "                                   )\n",
    "        return np.array(self.state), reward, done, False, {}\n",
    "\n",
    "    def reward(self, state):\n",
    "        if state == (0, 3):\n",
    "            return 1, True  # Goal state with reward\n",
    "        elif state == (1, 3):\n",
    "            return -1, True  # Failure state with negative reward\n",
    "        else:\n",
    "            return 0, False  # No reward, episode continues\n",
    "\n",
    "# Check the environment to ensure compatibility\n",
    "env = CustomEnv()\n",
    "check_env(env)\n",
    "\n",
    "# Create the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "# Test the trained model\n",
    "obs = env.reset()[0]\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    print(f\"State: {obs}, Reward: {reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = np.zeros((3,4))\n",
    "policy = np.zeros((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "class Actions(Enum):\n",
    "    up = 0,\n",
    "    right = 1,\n",
    "    down = 2,\n",
    "    left  = 3,\n",
    "    none = -1\n",
    "\n",
    "class env():\n",
    "    \n",
    "    actions = (Actions.up, Actions.right, Actions.down, Actions.left)\n",
    "    states = (\n",
    "            ((0,0), (0,1), (0,2), (0,3)),\n",
    "            ((1,0), (1,1), (1,2), (1,3)),\n",
    "            ((2,0), (2,1), (2,2), (2,3))\n",
    "    )\n",
    "    \n",
    "    iter_states = (\n",
    "            (0,0), (0,1), (0,2), (0,3),\n",
    "            (1,0), (1,1), (1,2), (1,3),\n",
    "            (2,0), (2,1), (2,2), (2,3)\n",
    "    )\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def step(self, state, action):\n",
    "        y, x = state\n",
    "        \n",
    "        if action == Actions.up:\n",
    "            y += 1\n",
    "        if action == Actions.right:\n",
    "            x += 1\n",
    "        if action == Actions.down:\n",
    "            y -= 1\n",
    "        if action == Actions.left:\n",
    "            x -= 1\n",
    "\n",
    "        y, x = max(y,0), max(x,0)\n",
    "        y, x = min(y,2), min(x,3)\n",
    "        state_next = (y,x)\n",
    "        reward, done = self.reward((y,x))\n",
    "        return state_next, reward, done\n",
    "        \n",
    "    def reward(self, next_state):\n",
    "        if next_state == (0,3):\n",
    "            return 1, True\n",
    "        elif next_state == (1,3):\n",
    "            return -1, True\n",
    "        else:\n",
    "            return 0, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 3\n",
    "gamma = 0.9\n",
    "e = env()\n",
    "for i in range(H):\n",
    "    for state in env.iter_states:\n",
    "        \n",
    "        if state == (0, 3) or state == (1, 3):\n",
    "            continue\n",
    "        \n",
    "        if state == (1, 1):\n",
    "            continue\n",
    "        \n",
    "        max_reward = 0\n",
    "        p = Actions.none.value\n",
    "        for idx, action in enumerate(env.actions):\n",
    "            n_state, rew = e.step(state, action)\n",
    "            n = rew + gamma * value[n_state]\n",
    "            if n > max_reward:\n",
    "                max_reward = n\n",
    "                p = action.value[0]\n",
    "        policy[state] = p\n",
    "        value[state] = max_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81  , 0.9   , 1.    , 0.    ],\n",
       "       [0.729 , 0.    , 0.9   , 0.    ],\n",
       "       [0.6561, 0.729 , 0.81  , 0.729 ]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 0.],\n",
       "       [2., 0., 2., 0.],\n",
       "       [1., 1., 2., 3.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy\n",
    "\n",
    "class Actions(Enum):\n",
    "    down = 0,\n",
    "    right = 1,\n",
    "    up = 2,\n",
    "    left  = 3,\n",
    "    none = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megatron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
