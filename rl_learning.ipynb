{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from gymnasium import Env, spaces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tiny_shakespeare import TinyShakespeareDataset\n",
    "from token_level_ppo import TokenLevelPPO\n",
    "from thoughtsformer_policy import ThoughtsFormerPolicy\n",
    "from word_embeddings import get_word_embeddings\n",
    "\n",
    "def token_batched_reshape_with_offset(x: torch.Tensor, max_seq_length: int, thoughts_taken: int) -> torch.Tensor:\n",
    "    thoughts = thoughts_taken + 1\n",
    "    max_thoughts = x.size(1) // max_seq_length\n",
    "    x = x[:,:max_seq_length*thoughts].view(x.size(0), max_seq_length, thoughts)\n",
    "    return F.pad(x,(0, (max_thoughts - thoughts)))\n",
    "        \n",
    "class ThoughtsFormerEnv(Env):\n",
    "    def __init__(self, vocab_size, max_sequence_length, max_thought_length):\n",
    "        super(ThoughtsFormerEnv, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_sequence_length = max_sequence_length \n",
    "        self.max_thought_length = max_thought_length\n",
    "        self.max_context_length = max_sequence_length * (max_thought_length+1)\n",
    "        \n",
    "        # Logits\n",
    "        self.action_space = spaces.MultiDiscrete([vocab_size] * self.max_sequence_length)\n",
    " \n",
    "        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"state\" : spaces.MultiDiscrete([vocab_size] * self.max_context_length),\n",
    "            \"thought_step\" : spaces.Discrete(max_thought_length+1)\n",
    "        })\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dataset = TinyShakespeareDataset(max_sequence_length,window_offset=max_sequence_length//4)\n",
    "        self.dataset_len = len(self.dataset)\n",
    "        self.dataset_iter = 0\n",
    "        self.thought_step = 0\n",
    "        \n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)  # Ensures Gymnasium's seeding is properly handled\n",
    "        \n",
    "        self.state, self.labels = self.dataset[self.dataset_iter]\n",
    "        if self.labels.ndim == 1:\n",
    "            self.labels = self.labels.unsqueeze(0)\n",
    "        self.state = F.pad(self.state, (0,self.max_context_length-self.max_sequence_length))\n",
    "     \n",
    "        # prepare self.state and massively elongate\n",
    "        self.dataset_iter += 1\n",
    "        if self.dataset == self.dataset_len:\n",
    "            self.dataset_iter = 0\n",
    "\n",
    "        self.thought_step = 0\n",
    "        \n",
    "        obs = {\n",
    "            'state' : self.state.numpy(),\n",
    "            'thought_step' : self.thought_step\n",
    "        }\n",
    "        return obs, {}\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        self.state = self.state.view(1,-1)\n",
    "        sampled_tokens = torch.from_numpy(action.reshape(1,-1))\n",
    "        \n",
    "        \n",
    "        if self.thought_step == self.max_thought_length:\n",
    "            reward = self.reward(sampled_tokens).flatten() \n",
    "            # print(reward.shape)\n",
    "            done = True\n",
    "        else:\n",
    "            reward = torch.zeros(self.max_sequence_length)\n",
    "            done = False\n",
    "            # Add the thought!\n",
    "            self.state = token_batched_reshape_with_offset(self.state, self.max_sequence_length, self.thought_step) # (batch x max_seq_len, (max_thought_len+1))\n",
    "            # before\n",
    "            # plt.imshow(self.state[0,:10]); plt.show()\n",
    "            \n",
    "            self.state[:,:,self.thought_step+1] = sampled_tokens\n",
    "            # plt.imshow(self.state[0,:10]); plt.show()\n",
    "            self.state =  self.state.view(1,-1)\n",
    "        \n",
    "        self.state = self.state.view(-1)\n",
    "        self.thought_step += 1\n",
    "        obs = {\n",
    "            'state' : self.state.numpy(),\n",
    "            'thought_step' : self.thought_step\n",
    "        }\n",
    "        \n",
    "        info = {'reward' : reward} \n",
    "        \n",
    "        # print(info)\n",
    "        \n",
    "        return obs, -np.inf, done, False, info #obs, reward, done, truncated, info\n",
    "    \n",
    "    def reward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        a, b = get_word_embeddings(tokens), get_word_embeddings(self.labels)\n",
    "        x = F.cosine_similarity(a, b, dim=-1) \n",
    "        x = torch.maximum(x, torch.Tensor([0]))\n",
    "        return x.detach()\n",
    "    # Encourages greater cosine similarity between tokens.\n",
    "    # def reward(self, action):\n",
    "    #     return -F.cross_entropy(torch.tensor(action), self.labels, reduction='none')\n",
    "    \n",
    "# Check the environment to ensure compatibility\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Now continue with your script\n",
    "\n",
    "env = ThoughtsFormerEnv(vocab_size=50257, max_sequence_length=256,max_thought_length=3)\n",
    "policy_kwargs = {\n",
    "    \"max_thought_len\" : 3,\n",
    "    \"from_gpt2\" : True\n",
    "}\n",
    "ppo = TokenLevelPPO(ThoughtsFormerPolicy, env, n_steps=4, batch_size=4, max_sequence_length=256, verbose=2, ent_coef=0.01, policy_kwargs=policy_kwargs)\n",
    "\n",
    "\n",
    "# ppo.learn(1, log_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object policy_class. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: Can't get attribute 'ThoughtsFormerPolicy' on <module 'token_level_ppo' from 'c:\\\\Users\\\\bicke\\\\Documents\\\\GitHub\\\\Thoughtsformer\\\\token_level_ppo.py'>\n",
      "  warnings.warn(\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    " \n",
    "policy_kwargs = {\n",
    "    \"max_thought_len\" : 1,\n",
    "    \"from_gpt2\" : True\n",
    "}\n",
    "\n",
    "env = ThoughtsFormerEnv(vocab_size=50257, max_sequence_length=512,max_thought_length=1)\n",
    "ppo = TokenLevelPPO.load(\"5600_timesteps.zip\", env=env, max_sequence_length=512, verbose=2, ent_coef=0.001,policy_kwargs=policy_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Token | Intermediate Thought | Prediction\n",
      "---------------------------------------------------\n",
      "First          |  .                   |   \\n        \n",
      "Citizen        |  duct                |   .         \n",
      ":              |  \\n                  |   \\n        \n",
      "\\n             |  Gamb                |   is        \n",
      "Before         |  ello                |   the       \n",
      "we             |  Grab                |   have      \n",
      "proceed        |  gee                 |   .         \n",
      "any            |  iman                |   This      \n",
      "further        |  genital             |   ,         \n",
      ",              |  benefits            |   as        \n",
      "hear           |  ance                |   it        \n",
      "me             |  puberty             |   have      \n",
      "speak          |  am                  |   .         \n",
      ".              |  032                 |   \\n        \n",
      "\\n             |  outsourcing         |   \\n        \n",
      "\\n             |  pulling             |   We        \n",
      "All            |  ponies              |   is        \n",
      ":              |  drawn               |   an        \n",
      "\\n             |  fucking             |   It        \n",
      "Spe            |  uries               |   .         \n",
      "ak             |  Fallon              |   on        \n",
      ",              |  Black               |   this      \n",
      "speak          |  thinkable           |   .         \n",
      ".              |  leak                |   \\n        \n",
      "\\n             |  envision            |   \\n        \n",
      "\\n             |  cler                |   \\n        \n",
      "First          |  tremendously        |   The       \n",
      "Citizen        |  188                 |   :         \n",
      ":              |  harassed            |   \\n        \n",
      "\\n             |  J                   |   It        \n",
      "You            |  creed               |   has       \n",
      "are            |  sleepy              |   in        \n",
      "all            |  flung               |   the       \n",
      "resolved       |  Laf                 |   ,         \n",
      "rather         |  dorm                |   is        \n",
      "to             |  aram                |   have      \n",
      "die            |  Engels              |   ,         \n",
      "than           |  Neighbor            |   the       \n",
      "to             |  puzzled             |   the       \n",
      "fam            |  Kurt                |   which     \n",
      "ish            |  medicines           |   ,         \n",
      "?              |  0000                |   \\n        \n",
      "\\n             |  Mitchell            |   \\n        \n",
      "\\n             |  Sylv                |   The       \n",
      "All            |  moral               |   to        \n",
      ":              |  Inside              |   The       \n",
      "\\n             |  heimer              |   The       \n",
      "Res            |  buses               |   281       \n",
      "olved          |  ecology             |   \\n        \n",
      ".              |  anova               |   \\n        \n",
      "resolved       |  listener            |   \\n        \n",
      ".              |  automatically       |   \\n        \n",
      "\\n             |  lator               |   \\n        \n",
      "\\n             |  Goes                |   It        \n",
      "First          |  ven                 |   It        \n",
      "Citizen        |  Violent             |   :         \n",
      ":              |  FINEST              |   \\n        \n",
      "\\n             |  \\n                  |   What      \n",
      "First          |  ink                 |   Even      \n",
      ",              |  contemplate         |   It        \n",
      "you            |  could               |   have      \n",
      "know           |  this                |   the       \n",
      "C              |  resident            |   468       \n",
      "ai             |  tastes              |   hath      \n",
      "us             |  BW                  |   .         \n",
      "Mar            |  Wall                |   There     \n",
      "cius           |  ocial               |   ,         \n",
      "is             |  Thai                |   that      \n",
      "chief          |  sip                 |   of        \n",
      "enemy          |  band                |   .         \n",
      "to             |  officer             |   And       \n",
      "the            |  actually            |   When      \n",
      "people         |  outrage             |   ,         \n",
      ".              |  ml                  |   \\n        \n",
      "\\n             |  mater               |   That      \n",
      "\\n             |  do                  |   When      \n",
      "All            |  Sok                 |   The       \n",
      ":              |  lack                |   in        \n",
      "\\n             |  runner              |   It        \n",
      "We             |  Herald              |   have      \n",
      "know           |  letter              |   it        \n",
      "'t             |  pict                |   a         \n",
      ",              |  ba                  |   in        \n",
      "we             |  suffer              |   are       \n",
      "know           |  demographics        |   the       \n",
      "'t             |  stage               |   as        \n",
      ".              |  Fitness             |   in        \n",
      "\\n             |  Sword               |   \\n        \n",
      "\\n             |  psychiat            |   The       \n",
      "First          |  And                 |   It        \n",
      "Citizen        |  clergy              |   :         \n",
      ":              |  Patterns            |   \\n        \n",
      "\\n             |  505                 |   What      \n",
      "Let            |  rescuing            |   you       \n",
      "us             |  machinery           |   have      \n",
      "kill           |  Static              |   the       \n",
      "him            |  Ascend              |   .         \n",
      ",              |  Doors               |   The       \n",
      "and            |  Ev                  |   The       \n",
      "we             |  breaths             |   will      \n"
     ]
    }
   ],
   "source": [
    "s = F.pad(env.dataset[0][0], (0,env.max_context_length-env.max_sequence_length))\n",
    "\n",
    "obs = {\n",
    "    'state' : s.reshape(1,-1).to('cuda'),\n",
    "    'thought_step' : 0\n",
    "\n",
    "}\n",
    "sampled_tokens, final_logits = ppo.policy.model.entire_thought_generation(obs['state'], torch.zeros_like(s).view(1,-1).to('cuda'))\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "p = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "y = ppo.policy.model._sample_tokens(final_logits)[0]\n",
    "\n",
    "def process(s):\n",
    "  return str(s).replace(\"\\n\", \"\\\\n\").strip()\n",
    "print(f\"{str('Original Token'):13} | {str('Intermediate Thought'):16} | Prediction\")\n",
    "print(\"---------------------------------------------------\")\n",
    "for i in range(100):\n",
    "  print(f\"\"\"\\\n",
    "{process(p.decode(sampled_tokens[0][2*i])):13}  |  {process(p.decode(sampled_tokens[0][2*i+1])):19} |   {process(p.decode(y[0][i])):10}\"\"\")\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "x = TinyShakespeareDataset(512,1024*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {'state' : x[0][0].view(1,-1).to('cuda'), 'thought_step' : 1}\n",
    "a,b = ppo.policy.forward(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0055, 0.0048, 0.0055,  ..., 0.0021, 0.0021, 0.0021],\n",
       "         [0.0047, 0.0052, 0.0047,  ..., 0.0024, 0.0024, 0.0024],\n",
       "         [0.0053, 0.0051, 0.0053,  ..., 0.0027, 0.0027, 0.0027],\n",
       "         ...,\n",
       "         [0.0104, 0.0040, 0.0104,  ..., 0.0233, 0.0233, 0.0233],\n",
       "         [0.0106, 0.0039, 0.0106,  ..., 0.0228, 0.0228, 0.0228],\n",
       "         [0.0110, 0.0038, 0.0110,  ..., 0.0236, 0.0236, 0.0236]]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = F.softmax(a,dim=-1)\n",
    "n[:,:,n.argmax(dim=-1).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "p = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.name_to_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ThoughtsFormerEnv.__init__() missing 3 required positional arguments: 'vocab_size', 'max_sequence_length', and 'max_thought_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_value_from_obs\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(obs)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 20\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mThoughtsFormerEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: ThoughtsFormerEnv.__init__() missing 3 required positional arguments: 'vocab_size', 'max_sequence_length', and 'max_thought_length'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from thoughtsformer import ThoughtsFormer, simple_batched_reshape_with_offset\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "class _ThoughtsFormerPolicy(ActorCriticPolicy):\n",
    "    def __init__(self):\n",
    "        self.model = ThoughtsFormer.from_pretrained_GPT2()\n",
    "        \n",
    "    def forward(self, obs: dict):\n",
    "        assert 'state' in obs and 'thought_step' in obs, f\"'state' and 'thought_step' should be keys of the observation dictionary\"\n",
    "        return self.model.forward_ppo_with_tokens(obs['state'], torch.zeros_like(obs['state']), obs['thought_step'])\n",
    "    \n",
    "    def _get_action_dist_from_obs(self, obs):\n",
    "        return self.forward(obs)[0]\n",
    "\n",
    "    def _get_value_from_obs(self, obs):\n",
    "        return self.forward(obs)[1]\n",
    "    \n",
    "env = ThoughtsFormerEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state torch.Size([11264])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TokenLevelPPO.collect_rollouts() got an unexpected keyword argument 'n_rollout_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bicke\\anaconda3\\envs\\megatron\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: TokenLevelPPO.collect_rollouts() got an unexpected keyword argument 'n_rollout_steps'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the environment to ensure compatibility\n",
    "from token_level_ppo import TokenLevelPPO, TokenLevelRolloutBuffer, ThoughtsFormerPolicy\n",
    "ppo = TokenLevelPPO(ThoughtsFormerPolicy, env, n_steps=2, batch_size=2, max_sequence_length=512, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 0, 0],\n",
       "         [3, 4, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "x = torch.tensor([1,2,3,4,0,0,0,0,0,0,0,0,0,0,0,0]).view(1,-1)\n",
    "\n",
    "\n",
    "def simple_batched_reshape_with_offset(x: torch.Tensor, max_seq_length: int, thoughts_taken: int) -> torch.Tensor:\n",
    "    thoughts = thoughts_taken + 1\n",
    "    max_thoughts = x.size(1) // max_seq_length\n",
    "    x = x[:,:max_seq_length*thoughts].view(x.size(0), max_seq_length, thoughts)\n",
    "    return F.pad(x,(0, (max_thoughts - thoughts)))\n",
    "\n",
    "simple_batched_reshape_with_offset(x,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.4     |\n",
      "|    ep_rew_mean     | -0.292   |\n",
      "| time/              |          |\n",
      "|    fps             | 327      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 32       |\n",
      "|    ep_rew_mean     | 0.524    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.4     |\n",
      "|    ep_rew_mean     | 0.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [2 2], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [2 0], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.9     |\n",
      "|    ep_rew_mean     | 0.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [2 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.5     |\n",
      "|    ep_rew_mean     | 0.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [2 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [2 0], Reward: 0\n",
      "State: [2 1], Reward: 0\n",
      "State: [1 1], Reward: 0\n",
      "State: [1 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.91     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [2 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.79     |\n",
      "|    ep_rew_mean     | 0.98     |\n",
      "| time/              |          |\n",
      "|    fps             | 355      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [2 0], Reward: 0\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.74     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 353      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.92     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 349      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.14     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "State: [1 0], Reward: 0\n",
      "State: [0 0], Reward: 0\n",
      "State: [0 1], Reward: 0\n",
      "State: [0 2], Reward: 0\n",
      "State: [0 3], Reward: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from gymnasium import Env, spaces\n",
    "\n",
    "class CustomEnv(Env):\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(4)  # 4 actions (up, right, down, left)\n",
    "        self.observation_space = spaces.MultiDiscrete([3, 4])  # (y, x) coordinates\n",
    "        self.state = (2, 0)  # Initial state\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)  # Ensures Gymnasium's seeding is properly handled\n",
    "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
    "        self.state = (2, 0)\n",
    "        return np.array(self.state), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action in the environment.\"\"\"\n",
    "        y, x = self.state\n",
    "\n",
    "        if action == 0:  # up\n",
    "            y += 1\n",
    "        elif action == 1:  # right\n",
    "            x += 1\n",
    "        elif action == 2:  # down\n",
    "            y -= 1\n",
    "        elif action == 3:  # left\n",
    "            x -= 1\n",
    "\n",
    "        # Keep coordinates within bounds\n",
    "        y, x = max(0, min(2, y)), max(0, min(3, x))\n",
    "        self.state = (y, x)\n",
    "\n",
    "        # Compute reward and done\n",
    "        reward, done = self.reward(self.state\n",
    "                                   )\n",
    "        return np.array(self.state), reward, done, False, {}\n",
    "\n",
    "    def reward(self, state):\n",
    "        if state == (0, 3):\n",
    "            return 1, True  # Goal state with reward\n",
    "        elif state == (1, 3):\n",
    "            return -1, True  # Failure state with negative reward\n",
    "        else:\n",
    "            return 0, False  # No reward, episode continues\n",
    "\n",
    "# Check the environment to ensure compatibility\n",
    "env = CustomEnv()\n",
    "check_env(env)\n",
    "\n",
    "# Create the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "# Test the trained model\n",
    "obs = env.reset()[0]\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    print(f\"State: {obs}, Reward: {reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = np.zeros((3,4))\n",
    "policy = np.zeros((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "class Actions(Enum):\n",
    "    up = 0,\n",
    "    right = 1,\n",
    "    down = 2,\n",
    "    left  = 3,\n",
    "    none = -1\n",
    "\n",
    "class env():\n",
    "    \n",
    "    actions = (Actions.up, Actions.right, Actions.down, Actions.left)\n",
    "    states = (\n",
    "            ((0,0), (0,1), (0,2), (0,3)),\n",
    "            ((1,0), (1,1), (1,2), (1,3)),\n",
    "            ((2,0), (2,1), (2,2), (2,3))\n",
    "    )\n",
    "    \n",
    "    iter_states = (\n",
    "            (0,0), (0,1), (0,2), (0,3),\n",
    "            (1,0), (1,1), (1,2), (1,3),\n",
    "            (2,0), (2,1), (2,2), (2,3)\n",
    "    )\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def step(self, state, action):\n",
    "        y, x = state\n",
    "        \n",
    "        if action == Actions.up:\n",
    "            y += 1\n",
    "        if action == Actions.right:\n",
    "            x += 1\n",
    "        if action == Actions.down:\n",
    "            y -= 1\n",
    "        if action == Actions.left:\n",
    "            x -= 1\n",
    "\n",
    "        y, x = max(y,0), max(x,0)\n",
    "        y, x = min(y,2), min(x,3)\n",
    "        state_next = (y,x)\n",
    "        reward, done = self.reward((y,x))\n",
    "        return state_next, reward, done\n",
    "        \n",
    "    def reward(self, next_state):\n",
    "        if next_state == (0,3):\n",
    "            return 1, True\n",
    "        elif next_state == (1,3):\n",
    "            return -1, True\n",
    "        else:\n",
    "            return 0, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 3\n",
    "gamma = 0.9\n",
    "e = env()\n",
    "for i in range(H):\n",
    "    for state in env.iter_states:\n",
    "        \n",
    "        if state == (0, 3) or state == (1, 3):\n",
    "            continue\n",
    "        \n",
    "        if state == (1, 1):\n",
    "            continue\n",
    "        \n",
    "        max_reward = 0\n",
    "        p = Actions.none.value\n",
    "        for idx, action in enumerate(env.actions):\n",
    "            n_state, rew = e.step(state, action)\n",
    "            n = rew + gamma * value[n_state]\n",
    "            if n > max_reward:\n",
    "                max_reward = n\n",
    "                p = action.value[0]\n",
    "        policy[state] = p\n",
    "        value[state] = max_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81  , 0.9   , 1.    , 0.    ],\n",
       "       [0.729 , 0.    , 0.9   , 0.    ],\n",
       "       [0.6561, 0.729 , 0.81  , 0.729 ]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 0.],\n",
       "       [2., 0., 2., 0.],\n",
       "       [1., 1., 2., 3.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy\n",
    "\n",
    "class Actions(Enum):\n",
    "    down = 0,\n",
    "    right = 1,\n",
    "    up = 2,\n",
    "    left  = 3,\n",
    "    none = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Janky token_level_ppo load in for when the environment and the previous weights dont match\n",
    "\n",
    "import warnings\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from stable_baselines3.common.buffers import DictRolloutBuffer\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from gymnasium import Env, spaces\n",
    "\n",
    "import torch\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "\n",
    "class TokenLevelRolloutBuffer(DictRolloutBuffer):\n",
    "    def __init__(self, *args, max_sequence_length: int, **kwargs) -> None:\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        self.rewards = np.zeros((self.buffer_size, self.n_envs, self.max_sequence_length), dtype=np.float32)\n",
    "        self.values = np.zeros_like(self.rewards)\n",
    "        self.actions = np.zeros_like(self.rewards) \n",
    "        self.advantages = np.zeros_like(self.rewards)\n",
    "        self.log_probs = np.zeros_like(self.rewards)\n",
    "    def add(  # type: ignore[override]\n",
    "        self,\n",
    "        obs: dict[str, np.ndarray],\n",
    "        action: np.ndarray,\n",
    "        reward: np.ndarray,\n",
    "        episode_start: np.ndarray,\n",
    "        value: torch.Tensor,\n",
    "        log_prob: torch.Tensor,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param obs: Observation\n",
    "        :param action: Action\n",
    "        :param reward:\n",
    "        :param episode_start: Start of episode signal.\n",
    "        :param value: estimated value of the current state\n",
    "            following the current policy.\n",
    "        :param log_prob: log probability of the action\n",
    "            following the current policy.\n",
    "        \"\"\"\n",
    "        if len(log_prob.shape) == 0:\n",
    "            # Reshape 0-d tensor to avoid error\n",
    "            log_prob = log_prob.reshape(-1, 1)\n",
    "\n",
    "        for key in self.observations.keys():\n",
    "            obs_ = np.array(obs[key])\n",
    "            # Reshape needed when using multiple envs with discrete observations\n",
    "            # as numpy cannot broadcast (n_discrete,) to (n_discrete, 1)\n",
    "            if isinstance(self.observation_space.spaces[key], spaces.Discrete):\n",
    "                obs_ = obs_.reshape((self.n_envs,) + self.obs_shape[key])\n",
    "            self.observations[key][self.pos] = obs_\n",
    "\n",
    "        # Reshape to handle multi-dim and discrete action spaces, see GH #970 #1392\n",
    "        action = action.reshape((self.n_envs, self.max_sequence_length)) # CHANGE IN EMMETT'S CODE (kinda janky change)\n",
    "        log_prob = log_prob.reshape((self.n_envs, self.action_dim)) # CHANGE IN EMMETT'S CODE \n",
    "        self.actions[self.pos] = np.array(action)\n",
    "        self.rewards[self.pos] = np.array(reward)\n",
    "        self.episode_starts[self.pos] = np.array(episode_start)\n",
    "        self.values[self.pos] = value.clone().cpu().numpy().flatten()\n",
    "        self.log_probs[self.pos] = log_prob.clone().cpu().numpy() # CHANGE IN EMMETT'S CODE \n",
    "        self.pos += 1\n",
    "        if self.pos == self.buffer_size:\n",
    "            self.full = True\n",
    "\n",
    "    def compute_returns_and_advantage(self, last_values: torch.Tensor | np.ndarray, dones: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Adapted for token-level GAE\n",
    "        \"\"\"\n",
    "        if isinstance(last_values, torch.Tensor):\n",
    "            last_values = last_values.cpu().numpy() \n",
    "        last_gae_lam = 0\n",
    "        for step in reversed(range(self.buffer_size)):\n",
    "            if step == self.buffer_size - 1:\n",
    "                next_non_terminal = 1.0 - dones\n",
    "                next_values = last_values\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - self.episode_starts[step + 1]\n",
    "                next_values = self.values[step + 1]\n",
    "            \n",
    "            delta = self.rewards[step] + self.gamma * next_values * next_non_terminal - self.values[step]\n",
    "            last_gae_lam = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae_lam\n",
    "            self.advantages[step] = last_gae_lam\n",
    "\n",
    "        # Reshape advantages and returns to match token-level structure\n",
    "        self.advantages = self.advantages.reshape(self.buffer_size, -1, self.max_sequence_length)\n",
    "        self.returns = self.advantages + self.values\n",
    "\n",
    "class TokenLevelPPO(PPO):\n",
    "    def __init__(self, policy, env, max_sequence_length, **kwargs):\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        super().__init__(policy, env, **kwargs)\n",
    "        self.rollout_buffer = TokenLevelRolloutBuffer(\n",
    "            self.n_steps,\n",
    "            self.observation_space,\n",
    "            self.action_space,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            device=self.device,\n",
    "            gae_lambda=self.gae_lambda,\n",
    "            gamma=self.gamma,\n",
    "            n_envs=self.n_envs,\n",
    "        )\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def load(  # noqa: C901\n",
    "        cls,\n",
    "        path,\n",
    "        max_sequence_length,\n",
    "        env = None,\n",
    "        device = \"auto\",\n",
    "        custom_objects = None,\n",
    "        print_system_info = False,\n",
    "        force_reset = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        from stable_baselines3.common.preprocessing import check_for_nested_spaces, is_image_space, is_image_space_channels_first\n",
    "        from stable_baselines3.common.save_util import load_from_zip_file, recursive_getattr, recursive_setattr, save_to_zip_file\n",
    "        from stable_baselines3.common.vec_env.patch_gym import _convert_space, _patch_env\n",
    "        from stable_baselines3.common.utils import (\n",
    "            check_for_correct_spaces,\n",
    "            get_device,\n",
    "            get_schedule_fn,\n",
    "            get_system_info,\n",
    "            set_random_seed,\n",
    "            update_learning_rate,\n",
    "        )\n",
    "        import warnings\n",
    "        \"\"\"\n",
    "        Load the model from a zip-file.\n",
    "        Warning: ``load`` re-creates the model from scratch, it does not update it in-place!\n",
    "        For an in-place load use ``set_parameters`` instead.\n",
    "\n",
    "        :param path: path to the file (or a file-like) where to\n",
    "            load the agent from\n",
    "        :param env: the new environment to run the loaded model on\n",
    "            (can be None if you only need prediction from a trained model) has priority over any saved environment\n",
    "        :param device: Device on which the code should run.\n",
    "        :param custom_objects: Dictionary of objects to replace\n",
    "            upon loading. If a variable is present in this dictionary as a\n",
    "            key, it will not be deserialized and the corresponding item\n",
    "            will be used instead. Similar to custom_objects in\n",
    "            ``keras.models.load_model``. Useful when you have an object in\n",
    "            file that can not be deserialized.\n",
    "        :param print_system_info: Whether to print system info from the saved model\n",
    "            and the current system info (useful to debug loading issues)\n",
    "        :param force_reset: Force call to ``reset()`` before training\n",
    "            to avoid unexpected behavior.\n",
    "            See https://github.com/DLR-RM/stable-baselines3/issues/597\n",
    "        :param kwargs: extra arguments to change the model when loading\n",
    "        :return: new model instance with loaded parameters\n",
    "        \"\"\"\n",
    "        if print_system_info:\n",
    "            print(\"== CURRENT SYSTEM INFO ==\")\n",
    "            get_system_info()\n",
    "\n",
    "        data, params, pytorch_variables = load_from_zip_file(\n",
    "            path,\n",
    "            device=device,\n",
    "            custom_objects=custom_objects,\n",
    "            print_system_info=print_system_info,\n",
    "        )\n",
    "\n",
    "        assert data is not None, \"No data found in the saved file\"\n",
    "        assert params is not None, \"No params found in the saved file\"\n",
    "\n",
    "        # Remove stored device information and replace with ours\n",
    "        if \"policy_kwargs\" in data:\n",
    "            if \"device\" in data[\"policy_kwargs\"]:\n",
    "                del data[\"policy_kwargs\"][\"device\"]\n",
    "            # backward compatibility, convert to new format\n",
    "            if \"net_arch\" in data[\"policy_kwargs\"] and len(data[\"policy_kwargs\"][\"net_arch\"]) > 0:\n",
    "                saved_net_arch = data[\"policy_kwargs\"][\"net_arch\"]\n",
    "                if isinstance(saved_net_arch, list) and isinstance(saved_net_arch[0], dict):\n",
    "                    data[\"policy_kwargs\"][\"net_arch\"] = saved_net_arch[0]\n",
    "\n",
    "        if \"policy_kwargs\" in kwargs and kwargs[\"policy_kwargs\"] != data[\"policy_kwargs\"]:\n",
    "            pass\n",
    "            # raise ValueError(\n",
    "            #     f\"The specified policy kwargs do not equal the stored policy kwargs.\"\n",
    "            #     f\"Stored kwargs: {data['policy_kwargs']}, specified kwargs: {kwargs['policy_kwargs']}\"\n",
    "            # )\n",
    "\n",
    "        if \"observation_space\" not in data or \"action_space\" not in data:\n",
    "            raise KeyError(\"The observation_space and action_space were not given, can't verify new environments\")\n",
    "\n",
    "        # Gym -> Gymnasium space conversion\n",
    "        for key in {\"observation_space\", \"action_space\"}:\n",
    "            data[key] = _convert_space(data[key])\n",
    "\n",
    "        if env is not None:\n",
    "            # Wrap first if needed\n",
    "            env = cls._wrap_env(env, data[\"verbose\"])\n",
    "            # Check if given env is valid\n",
    "            check_for_correct_spaces(env, data[\"observation_space\"], data[\"action_space\"])\n",
    "            # Discard `_last_obs`, this will force the env to reset before training\n",
    "            # See issue https://github.com/DLR-RM/stable-baselines3/issues/597\n",
    "            if force_reset and data is not None:\n",
    "                data[\"_last_obs\"] = None\n",
    "            # `n_envs` must be updated. See issue https://github.com/DLR-RM/stable-baselines3/issues/1018\n",
    "            if data is not None:\n",
    "                data[\"n_envs\"] = env.num_envs\n",
    "        else:\n",
    "            # Use stored env, if one exists. If not, continue as is (can be used for predict)\n",
    "            if \"env\" in data:\n",
    "                env = data[\"env\"]\n",
    "        from thoughtsformer_policy import ThoughtsFormerPolicy\n",
    "        model = cls(\n",
    "            policy=ThoughtsFormerPolicy,\n",
    "            env=env,\n",
    "            device=device,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            _init_setup_model=False,  # type: ignore[call-arg]\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # load parameters\n",
    "        model.__dict__.update(data)\n",
    "        model.__dict__.update(kwargs)\n",
    "        model._setup_model()\n",
    "\n",
    "        try:\n",
    "            # put state_dicts back in place\n",
    "            model.set_parameters(params, exact_match=True, device=device)\n",
    "        except RuntimeError as e:\n",
    "            # Patch to load Policy saved using SB3 < 1.7.0\n",
    "            # the error is probably due to old policy being loaded\n",
    "            # See https://github.com/DLR-RM/stable-baselines3/issues/1233\n",
    "            if \"pi_features_extractor\" in str(e) and \"Missing key(s) in state_dict\" in str(e):\n",
    "                model.set_parameters(params, exact_match=False, device=device)\n",
    "                warnings.warn(\n",
    "                    \"You are probably loading a model saved with SB3 < 1.7.0, \"\n",
    "                    \"we deactivated exact_match so you can save the model \"\n",
    "                    \"again to avoid issues in the future \"\n",
    "                    \"(see https://github.com/DLR-RM/stable-baselines3/issues/1233 for more info). \"\n",
    "                    f\"Original error: {e} \\n\"\n",
    "                    \"Note: the model should still work fine, this only a warning.\"\n",
    "                )\n",
    "            else:\n",
    "                raise e\n",
    "        # put other pytorch variables back in place\n",
    "        if pytorch_variables is not None:\n",
    "            for name in pytorch_variables:\n",
    "                # Skip if PyTorch variable was not defined (to ensure backward compatibility).\n",
    "                # This happens when using SAC/TQC.\n",
    "                # SAC has an entropy coefficient which can be fixed or optimized.\n",
    "                # If it is optimized, an additional PyTorch variable `log_ent_coef` is defined,\n",
    "                # otherwise it is initialized to `None`.\n",
    "                if pytorch_variables[name] is None:\n",
    "                    continue\n",
    "                # Set the data attribute directly to avoid issue when using optimizers\n",
    "                # See https://github.com/DLR-RM/stable-baselines3/issues/391\n",
    "                recursive_setattr(model, f\"{name}.data\", pytorch_variables[name].data)\n",
    "\n",
    "        # Sample gSDE exploration matrix, so it uses the right device\n",
    "        # see issue #44\n",
    "        if model.use_sde:\n",
    "            model.policy.reset_noise()  # type: ignore[operator]\n",
    "            \n",
    "            \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _setup_model(self) -> None:\n",
    "        super()._setup_model()\n",
    "        self.rollout_buffer = TokenLevelRolloutBuffer(\n",
    "            self.n_steps,\n",
    "            self.observation_space,\n",
    "            self.action_space,\n",
    "            max_sequence_length=self.max_sequence_length,\n",
    "            device=self.device,\n",
    "            gae_lambda=self.gae_lambda,\n",
    "            gamma=self.gamma,\n",
    "            n_envs=self.n_envs,\n",
    "        )\n",
    "        \n",
    "\n",
    "    def collect_rollouts(self, env, callback, rollout_buffer, n_rollout_steps):\n",
    "        # Reset or initialize variables\n",
    "        assert self._last_obs is not None, \"No previous observation was provided\"\n",
    "        # Switch to eval mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(False)\n",
    "        \n",
    "        rollout_buffer.reset()\n",
    "        for step in range(n_rollout_steps):\n",
    "            # Get actions and values from the policy\n",
    "            with torch.no_grad():\n",
    "                obs_tensor = obs_as_tensor(self._last_obs, self.device)\n",
    "                actions, values, log_probs = self.policy(obs_tensor)\n",
    "            actions = actions.cpu().numpy()\n",
    "\n",
    "\n",
    "            if isinstance(self.action_space, spaces.Box):\n",
    "                if self.policy.squash_output:\n",
    "                    # Unscale the actions to match env bounds\n",
    "                    # if they were previously squashed (scaled in [-1, 1])\n",
    "                    clipped_actions = self.policy.unscale_action(clipped_actions)\n",
    "                else:\n",
    "                    # Otherwise, clip the actions to avoid out of bound error\n",
    "                    # as we are sampling from an unbounded Gaussian distribution\n",
    "                    clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
    "\n",
    "            new_observations, rewards, dones, infos = env.step(actions)\n",
    "            rewards = torch.stack([infos[idx]['reward'] for idx in range(len(infos))]).numpy()\n",
    "            # actions = torch.stack([infos[idx]['actions_taken'] for idx in range(len(infos))]).numpy()\n",
    "            \n",
    "            self.num_timesteps += env.num_envs\n",
    "            \n",
    "            self._update_info_buffer(infos, dones)\n",
    "            \n",
    "            \n",
    "            for idx, done in enumerate(dones):\n",
    "                if (\n",
    "                    done\n",
    "                    and infos[idx].get(\"terminal_observation\") is not None\n",
    "                    and infos[idx].get(\"TimeLimit.truncated\", False)\n",
    "                ):\n",
    "                    terminal_obs = self.policy.obs_to_tensor(infos[idx][\"terminal_observation\"])[0]\n",
    "                    with torch.no_grad():\n",
    "                        terminal_value = self.policy.predict_values(terminal_obs)[0]  # type: ignore[arg-type]\n",
    "                    rewards[idx] += self.gamma * terminal_value\n",
    "\n",
    "  \n",
    "            \n",
    "            # Store data in the buffer\n",
    "            \n",
    "            rollout_buffer.add(\n",
    "                self._last_obs,  # type: ignore[arg-type]\n",
    "                actions,\n",
    "                rewards,\n",
    "                self._last_episode_starts,  # type: ignore[arg-type]\n",
    "                values,\n",
    "                log_probs)\n",
    "            \n",
    "            self._last_obs = new_observations  \n",
    "            self._last_episode_starts = dones\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            # Compute value for the last timestep\n",
    "            values = self.policy.predict_values(obs_as_tensor(new_observations, self.device))  # type: ignore[arg-type]\n",
    "            \n",
    "        rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
    "        return True\n",
    "        \n",
    "    def train(self) -> None:\n",
    "        counter = 0\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for rollout_data in self.rollout_buffer.get(self.batch_size):\n",
    "                \n",
    "                with torch.autograd.set_detect_anomaly(True):\n",
    "                    actions = rollout_data.actions\n",
    "                    actions = actions.reshape(actions.size(0), self.max_sequence_length).to(torch.long)\n",
    "                    # actions = actions.reshape(actions.size(0),self.\n",
    "                    rollout_data.observations['state'] = rollout_data.observations['state'].to(torch.long)\n",
    "                    # rollout_data.observations['thought_step'] \n",
    "                    values, action_log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
    "\n",
    "                    values = values.view(-1, actions.shape[1]) # (batch x tokens)\n",
    "                    # Normalize advantage\n",
    "                    advantages = rollout_data.advantages\n",
    "                    advantages = advantages.view(-1, actions.shape[1]) # (batch x tokens)\n",
    "                    if advantages.size(0) >= 3: # Only normalize if there's a sizable batch\n",
    "                        advantages = (advantages - advantages.mean(dim=0, keepdim=True)) / (advantages.std(dim=0, keepdim=True) + 1e-8)\n",
    "                    else:\n",
    "                        warnings.warn(\"Size of current batch is less than or equal to two, so advantage batch normalization has been disabled this iteration.\")\n",
    "\n",
    "                    \n",
    "                    # ratio between old and new policy, should be one at the first iteration\n",
    "                \n",
    "                    action_log_prob = action_log_prob.view(-1, actions.shape[1]) # what our policy thinks about the old action taken\n",
    "                    old_log_prob = rollout_data.old_log_prob.view(-1, actions.shape[1]) # what the old policy thinks about the action taken\n",
    "                    # old_log_prob = F.log_softmax(old_log_prob, dim=-1)\n",
    "                    # old_log_prob = old_log_prob.gather(-1, actions.unsqueeze(-1)).squeeze(-1) # FIX: Terrible extraction of old log prob for taken actions after the fact and requires storing an outrageous amount of information\n",
    "                    ratio = torch.exp(action_log_prob - old_log_prob)\n",
    "                    \n",
    "                    current_clip_range = self.clip_range(counter / self._total_timesteps)\n",
    "                    # clipped surrogate loss\n",
    "                    policy_loss_1 = advantages * ratio\n",
    "                    policy_loss_2 = advantages * torch.clamp(ratio, 1 - current_clip_range, 1 + current_clip_range)\n",
    "                    policy_loss = -torch.min(policy_loss_1, policy_loss_2)\n",
    "\n",
    "                    \n",
    "                    # Value clipping is omitted in the token-based version, but could be added if needed\n",
    "                    value_loss = F.mse_loss(rollout_data.returns.view(-1, actions.shape[1]), values, reduction='none')\n",
    "                    # Value loss using the TD(gae_lambda) target\n",
    "\n",
    "                    # Entropy loss favor exploration\n",
    "                    if entropy is None:\n",
    "                        entropy_loss = -action_log_prob\n",
    "                    else:\n",
    "                        entropy_loss = -entropy.view(-1, actions.shape[1])\n",
    "\n",
    "                    token_loss = policy_loss + self.vf_coef * value_loss + self.ent_coef * entropy_loss\n",
    "                    loss = token_loss.mean()\n",
    "                    \n",
    "                    self.policy.optimizer.zero_grad()\n",
    "                \n",
    "                    loss.backward()\n",
    "                # .backward()\n",
    "                # Clip grad norm\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.policy.optimizer.step()\n",
    "               \n",
    "                # Logging\n",
    "                counter+=1\n",
    "            \n",
    "                self.logger.record(\"train/loss\", loss.item())\n",
    "                self.logger.record(\"train/policy_loss\", policy_loss.mean().item())\n",
    "                self.logger.record(\"train/value_loss\", value_loss.mean().item())\n",
    "                self.logger.record(\"train/entropy_loss\", entropy_loss.mean().item())\n",
    "                self.logger.record(\"train/mean_reward\", rollout_data.returns.mean().item())\n",
    "                \n",
    "        self.logger.dump(step=counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megatron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
