{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from thoughtsformer import ThoughtsFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tiny_shakespeare import TinyShakespeareDataset, TokenizerType\n",
    "from character_tokenizer import ShakespeareCharacterTokenizer\n",
    "dataset_tr = TinyShakespeareDataset(128,64, split=\"train\", tokenizer=TokenizerType.CHARACTER_LEVEL)\n",
    "dataset_te = TinyShakespeareDataset(128,64, split=\"test\", tokenizer=TokenizerType.CHARACTER_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "max_sequence_length = 128\n",
    "vocab_size = 65\n",
    "num_layers = 1\n",
    "n_head = 6\n",
    "d_embed = 384\n",
    "dim_feedforward = d_embed\n",
    "dropout = 0\n",
    "\n",
    "def return_model_with_n_thoughts(n: int):\n",
    "  return ThoughtsFormer(\n",
    "      max_thought_len=n,\n",
    "      max_sequence_length=max_sequence_length,\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      n_head=n_head,\n",
    "      d_embed=d_embed,\n",
    "      dim_feedforward=dim_feedforward,\n",
    "      dropout=dropout\n",
    "  ).to(device)\n",
    "\n",
    "m0 = return_model_with_n_thoughts(0)\n",
    "# m1 = return_model_with_n_thoughts(3)\n",
    "m2 = return_model_with_n_thoughts(4)\n",
    "# m3 = return_model_with_n_thoughts(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dataset = TinyShakespeareDataset(128,64)\n",
    "train_size = len(dataset_tr)\n",
    "test_size = len(dataset_te) \n",
    "\n",
    "train_loader = DataLoader(dataset_tr, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(dataset_te, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thoughtsformer Train Loss at batch 0, epoch 0: 4.341577529907227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thoughtsformer Train Loss at batch 1, epoch 0: 4.265557289123535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thoughtsformer Train Loss at batch 2, epoch 0: 4.230602264404297\n",
      "Thoughtsformer Train Loss at batch 3, epoch 0: 4.1931939125061035\n",
      "Thoughtsformer Train Loss at batch 4, epoch 0: 4.193692207336426\n",
      "Thoughtsformer Train Loss at batch 5, epoch 0: 4.068216800689697\n",
      "Thoughtsformer Train Loss at batch 6, epoch 0: 3.946465015411377\n",
      "Thoughtsformer Train Loss at batch 7, epoch 0: 3.9958157539367676\n",
      "Thoughtsformer Train Loss at batch 8, epoch 0: 3.9159443378448486\n",
      "Thoughtsformer Train Loss at batch 9, epoch 0: 3.882925510406494\n",
      "Thoughtsformer Train Loss at batch 10, epoch 0: 3.7387924194335938\n",
      "Thoughtsformer Train Loss at batch 11, epoch 0: 3.7353692054748535\n",
      "Thoughtsformer Train Loss at batch 12, epoch 0: 3.696666955947876\n",
      "Thoughtsformer Train Loss at batch 13, epoch 0: 3.633847713470459\n",
      "Thoughtsformer Train Loss at batch 14, epoch 0: 3.6088967323303223\n",
      "Thoughtsformer Train Loss at batch 15, epoch 0: 3.497584342956543\n",
      "Thoughtsformer Train Loss at batch 16, epoch 0: 3.6218268871307373\n",
      "Thoughtsformer Train Loss at batch 17, epoch 0: 3.454017400741577\n",
      "Thoughtsformer Train Loss at batch 18, epoch 0: 3.440049409866333\n",
      "Thoughtsformer Train Loss at batch 19, epoch 0: 3.4447576999664307\n",
      "Thoughtsformer Train Loss at batch 20, epoch 0: 3.512840747833252\n",
      "Thoughtsformer Train Loss at batch 21, epoch 0: 3.3734309673309326\n",
      "Thoughtsformer Train Loss at batch 22, epoch 0: 3.4746782779693604\n",
      "Thoughtsformer Train Loss at batch 23, epoch 0: 3.385118007659912\n",
      "Thoughtsformer Train Loss at batch 24, epoch 0: 3.4678332805633545\n",
      "Thoughtsformer Train Loss at batch 25, epoch 0: 3.483215093612671\n",
      "Thoughtsformer Train Loss at batch 26, epoch 0: 3.4968667030334473\n",
      "Thoughtsformer Train Loss at batch 27, epoch 0: 3.472456455230713\n",
      "Thoughtsformer Train Loss at batch 28, epoch 0: 3.3901093006134033\n",
      "Thoughtsformer Train Loss at batch 29, epoch 0: 3.2692742347717285\n",
      "Thoughtsformer Train Loss at batch 30, epoch 0: 3.356915235519409\n",
      "Thoughtsformer Train Loss at batch 31, epoch 0: 3.215944290161133\n",
      "Thoughtsformer Train Loss at batch 32, epoch 0: 3.404996871948242\n",
      "Thoughtsformer Train Loss at batch 33, epoch 0: 3.4097471237182617\n",
      "Thoughtsformer Train Loss at batch 34, epoch 0: 3.329601287841797\n",
      "Thoughtsformer Train Loss at batch 35, epoch 0: 3.343263626098633\n",
      "Thoughtsformer Train Loss at batch 36, epoch 0: 3.264899253845215\n",
      "Thoughtsformer Train Loss at batch 37, epoch 0: 3.453690528869629\n",
      "Thoughtsformer Train Loss at batch 38, epoch 0: 3.283447742462158\n",
      "Thoughtsformer Train Loss at batch 39, epoch 0: 3.3146591186523438\n",
      "Thoughtsformer Train Loss at batch 40, epoch 0: 3.411932945251465\n",
      "Thoughtsformer Train Loss at batch 41, epoch 0: 3.434000253677368\n",
      "Thoughtsformer Train Loss at batch 42, epoch 0: 3.231764316558838\n",
      "Thoughtsformer Train Loss at batch 43, epoch 0: 3.3525891304016113\n",
      "Thoughtsformer Train Loss at batch 44, epoch 0: 3.3138301372528076\n",
      "Thoughtsformer Train Loss at batch 45, epoch 0: 3.2889010906219482\n",
      "Thoughtsformer Train Loss at batch 46, epoch 0: 3.36281681060791\n",
      "Thoughtsformer Train Loss at batch 47, epoch 0: 3.254067897796631\n",
      "Thoughtsformer Train Loss at batch 48, epoch 0: 3.270904541015625\n",
      "Thoughtsformer Train Loss at batch 49, epoch 0: 3.224233865737915\n",
      "Thoughtsformer Train Loss at batch 50, epoch 0: 3.4342808723449707\n",
      "Thoughtsformer Train Loss at batch 51, epoch 0: 3.1894805431365967\n",
      "Thoughtsformer Train Loss at batch 52, epoch 0: 3.685661792755127\n",
      "Thoughtsformer Train Loss at batch 53, epoch 0: 3.3082666397094727\n",
      "Thoughtsformer Train Loss at batch 54, epoch 0: 3.4951274394989014\n",
      "Thoughtsformer Train Loss at batch 55, epoch 0: 3.250338554382324\n",
      "Thoughtsformer Train Loss at batch 56, epoch 0: 3.3546347618103027\n",
      "Thoughtsformer Train Loss at batch 57, epoch 0: 3.1941707134246826\n",
      "Thoughtsformer Train Loss at batch 58, epoch 0: 3.3065500259399414\n",
      "Thoughtsformer Train Loss at batch 59, epoch 0: 3.359245777130127\n",
      "Thoughtsformer Train Loss at batch 60, epoch 0: 3.3780336380004883\n",
      "Thoughtsformer Train Loss at batch 61, epoch 0: 3.3075008392333984\n",
      "Thoughtsformer Train Loss at batch 62, epoch 0: 3.3089170455932617\n",
      "Thoughtsformer Train Loss at batch 63, epoch 0: 3.3403713703155518\n",
      "Thoughtsformer Train Loss at batch 64, epoch 0: 3.415574073791504\n",
      "Thoughtsformer Train Loss at batch 65, epoch 0: 3.424027919769287\n",
      "Thoughtsformer Train Loss at batch 66, epoch 0: 3.4291911125183105\n",
      "Thoughtsformer Train Loss at batch 67, epoch 0: 3.2826900482177734\n",
      "Thoughtsformer Train Loss at batch 68, epoch 0: 3.2134883403778076\n",
      "Thoughtsformer Train Loss at batch 69, epoch 0: 3.385946035385132\n",
      "Thoughtsformer Train Loss at batch 70, epoch 0: 3.427170515060425\n",
      "Thoughtsformer Train Loss at batch 71, epoch 0: 3.3170700073242188\n",
      "Thoughtsformer Train Loss at batch 72, epoch 0: 3.336421012878418\n",
      "Thoughtsformer Train Loss at batch 73, epoch 0: 3.338606119155884\n",
      "Thoughtsformer Train Loss at batch 74, epoch 0: 3.3193931579589844\n",
      "Thoughtsformer Train Loss at batch 75, epoch 0: 3.441220760345459\n",
      "Thoughtsformer Train Loss at batch 76, epoch 0: 3.192718267440796\n",
      "Thoughtsformer Train Loss at batch 77, epoch 0: 3.534040689468384\n",
      "Thoughtsformer Train Loss at batch 78, epoch 0: 3.351592540740967\n",
      "Thoughtsformer Train Loss at batch 79, epoch 0: 3.263460636138916\n",
      "Thoughtsformer Train Loss at batch 80, epoch 0: 3.347787857055664\n",
      "Thoughtsformer Train Loss at batch 81, epoch 0: 3.3179681301116943\n",
      "Thoughtsformer Train Loss at batch 82, epoch 0: 3.275635242462158\n",
      "Thoughtsformer Train Loss at batch 83, epoch 0: 3.2244932651519775\n",
      "Thoughtsformer Train Loss at batch 84, epoch 0: 3.2872133255004883\n",
      "Thoughtsformer Train Loss at batch 85, epoch 0: 3.25009822845459\n",
      "Thoughtsformer Train Loss at batch 86, epoch 0: 3.3819198608398438\n",
      "Thoughtsformer Train Loss at batch 87, epoch 0: 3.328099489212036\n",
      "Thoughtsformer Train Loss at batch 88, epoch 0: 3.232360363006592\n",
      "Thoughtsformer Train Loss at batch 89, epoch 0: 3.3413844108581543\n",
      "Thoughtsformer Train Loss at batch 90, epoch 0: 3.322327136993408\n",
      "Thoughtsformer Train Loss at batch 91, epoch 0: 3.345456600189209\n",
      "Thoughtsformer Train Loss at batch 92, epoch 0: 3.4414496421813965\n",
      "Thoughtsformer Train Loss at batch 93, epoch 0: 3.310927391052246\n",
      "Thoughtsformer Train Loss at batch 94, epoch 0: 3.4121222496032715\n",
      "Thoughtsformer Train Loss at batch 95, epoch 0: 3.2629048824310303\n",
      "Thoughtsformer Train Loss at batch 96, epoch 0: 3.3181819915771484\n",
      "Thoughtsformer Train Loss at batch 97, epoch 0: 3.3314309120178223\n",
      "Thoughtsformer Train Loss at batch 98, epoch 0: 3.438282012939453\n",
      "Thoughtsformer Train Loss at batch 99, epoch 0: 3.5382463932037354\n",
      "Thoughtsformer Train Loss at batch 100, epoch 0: 3.3041768074035645\n",
      "Thoughtsformer Train Loss at batch 101, epoch 0: 3.291332721710205\n",
      "Thoughtsformer Train Loss at batch 102, epoch 0: 3.313380718231201\n",
      "Thoughtsformer Train Loss at batch 103, epoch 0: 3.279733419418335\n",
      "Thoughtsformer Train Loss at batch 104, epoch 0: 3.4475677013397217\n",
      "Thoughtsformer Train Loss at batch 105, epoch 0: 3.403254508972168\n",
      "Thoughtsformer Train Loss at batch 106, epoch 0: 3.3103771209716797\n",
      "Thoughtsformer Train Loss at batch 107, epoch 0: 3.1938281059265137\n",
      "Thoughtsformer Train Loss at batch 108, epoch 0: 3.3193836212158203\n",
      "Thoughtsformer Train Loss at batch 109, epoch 0: 3.3249669075012207\n",
      "Thoughtsformer Train Loss at batch 110, epoch 0: 3.2545933723449707\n",
      "Thoughtsformer Train Loss at batch 111, epoch 0: 3.2212228775024414\n",
      "Thoughtsformer Train Loss at batch 112, epoch 0: 3.4933078289031982\n",
      "Thoughtsformer Train Loss at batch 113, epoch 0: 3.219177722930908\n",
      "Thoughtsformer Train Loss at batch 114, epoch 0: 3.1750898361206055\n",
      "Thoughtsformer Train Loss at batch 115, epoch 0: 3.5294485092163086\n",
      "Thoughtsformer Train Loss at batch 116, epoch 0: 3.353322982788086\n",
      "Thoughtsformer Train Loss at batch 117, epoch 0: 3.267955780029297\n",
      "Thoughtsformer Train Loss at batch 118, epoch 0: 3.1642305850982666\n",
      "Thoughtsformer Train Loss at batch 119, epoch 0: 3.4027323722839355\n",
      "Thoughtsformer Train Loss at batch 120, epoch 0: 3.391963481903076\n",
      "Thoughtsformer Train Loss at batch 121, epoch 0: 3.4958605766296387\n",
      "Thoughtsformer Train Loss at batch 122, epoch 0: 3.3279895782470703\n",
      "Thoughtsformer Train Loss at batch 123, epoch 0: 3.2613091468811035\n",
      "Thoughtsformer Train Loss at batch 124, epoch 0: 3.21758770942688\n",
      "Thoughtsformer Train Loss at batch 125, epoch 0: 3.499443531036377\n",
      "Thoughtsformer Train Loss at batch 126, epoch 0: 3.4265708923339844\n",
      "Thoughtsformer Train Loss at batch 127, epoch 0: 3.406691074371338\n",
      "Thoughtsformer Train Loss at batch 128, epoch 0: 3.375192642211914\n",
      "Thoughtsformer Train Loss at batch 129, epoch 0: 3.224534749984741\n",
      "Thoughtsformer Train Loss at batch 130, epoch 0: 3.4414658546447754\n",
      "Thoughtsformer Train Loss at batch 131, epoch 0: 3.2511096000671387\n",
      "Thoughtsformer Train Loss at batch 132, epoch 0: 3.553936004638672\n",
      "Thoughtsformer Train Loss at batch 133, epoch 0: 3.2684967517852783\n",
      "Thoughtsformer Train Loss at batch 134, epoch 0: 3.3021771907806396\n",
      "Thoughtsformer Train Loss at batch 135, epoch 0: 3.292710542678833\n",
      "Thoughtsformer Train Loss at batch 136, epoch 0: 3.2392098903656006\n",
      "Thoughtsformer Train Loss at batch 137, epoch 0: 3.3222134113311768\n",
      "Thoughtsformer Train Loss at batch 138, epoch 0: 3.4113683700561523\n",
      "Thoughtsformer Train Loss at batch 139, epoch 0: 3.343433141708374\n",
      "Thoughtsformer Train Loss at batch 140, epoch 0: 3.3446078300476074\n",
      "Thoughtsformer Train Loss at batch 141, epoch 0: 3.3777976036071777\n",
      "Thoughtsformer Train Loss at batch 142, epoch 0: 3.368973731994629\n",
      "Thoughtsformer Train Loss at batch 143, epoch 0: 3.2151498794555664\n",
      "Thoughtsformer Train Loss at batch 144, epoch 0: 3.276144504547119\n",
      "Thoughtsformer Train Loss at batch 145, epoch 0: 3.2812979221343994\n",
      "Thoughtsformer Train Loss at batch 146, epoch 0: 3.4318084716796875\n",
      "Thoughtsformer Train Loss at batch 147, epoch 0: 3.4374804496765137\n",
      "Thoughtsformer Train Loss at batch 148, epoch 0: 3.2459616661071777\n",
      "Thoughtsformer Train Loss at batch 149, epoch 0: 3.2496023178100586\n",
      "Thoughtsformer Train Loss at batch 150, epoch 0: 3.363233804702759\n",
      "Thoughtsformer Train Loss at batch 151, epoch 0: 3.3752152919769287\n",
      "Thoughtsformer Train Loss at batch 152, epoch 0: 3.229492664337158\n",
      "Thoughtsformer Train Loss at batch 153, epoch 0: 3.4585046768188477\n",
      "Thoughtsformer Train Loss at batch 154, epoch 0: 3.2332799434661865\n",
      "Thoughtsformer Train Loss at batch 155, epoch 0: 3.3663368225097656\n",
      "Thoughtsformer Train Loss at batch 156, epoch 0: 3.3721461296081543\n",
      "Thoughtsformer Train Loss at batch 157, epoch 0: 3.246870994567871\n",
      "Thoughtsformer Train Loss at batch 158, epoch 0: 3.3774912357330322\n",
      "Thoughtsformer Train Loss at batch 159, epoch 0: 3.2864158153533936\n",
      "Thoughtsformer Train Loss at batch 160, epoch 0: 3.2408008575439453\n",
      "Thoughtsformer Train Loss at batch 161, epoch 0: 3.344994068145752\n",
      "Thoughtsformer Train Loss at batch 162, epoch 0: 3.283402919769287\n",
      "Thoughtsformer Train Loss at batch 163, epoch 0: 3.2387962341308594\n",
      "Thoughtsformer Train Loss at batch 164, epoch 0: 3.3996806144714355\n",
      "Thoughtsformer Train Loss at batch 165, epoch 0: 3.36470890045166\n",
      "Thoughtsformer Train Loss at batch 166, epoch 0: 3.379601001739502\n",
      "Thoughtsformer Train Loss at batch 167, epoch 0: 3.386242389678955\n",
      "Thoughtsformer Train Loss at batch 168, epoch 0: 3.358489990234375\n",
      "Thoughtsformer Train Loss at batch 169, epoch 0: 3.3362016677856445\n",
      "Thoughtsformer Train Loss at batch 170, epoch 0: 3.2023186683654785\n",
      "Thoughtsformer Train Loss at batch 171, epoch 0: 3.365626335144043\n",
      "Thoughtsformer Train Loss at batch 172, epoch 0: 3.440377712249756\n",
      "Thoughtsformer Train Loss at batch 173, epoch 0: 3.573683023452759\n",
      "Thoughtsformer Train Loss at batch 174, epoch 0: 3.3528542518615723\n",
      "Thoughtsformer Train Loss at batch 175, epoch 0: 3.3411498069763184\n",
      "Thoughtsformer Train Loss at batch 176, epoch 0: 3.25656795501709\n",
      "Thoughtsformer Train Loss at batch 177, epoch 0: 3.3142290115356445\n",
      "Thoughtsformer Train Loss at batch 178, epoch 0: 3.307147264480591\n",
      "Thoughtsformer Train Loss at batch 179, epoch 0: 3.2399325370788574\n",
      "Thoughtsformer Train Loss at batch 180, epoch 0: 3.3750622272491455\n",
      "Thoughtsformer Train Loss at batch 181, epoch 0: 3.306878089904785\n",
      "Thoughtsformer Train Loss at batch 182, epoch 0: 3.1918833255767822\n",
      "Thoughtsformer Train Loss at batch 183, epoch 0: 3.2679781913757324\n",
      "Thoughtsformer Train Loss at batch 184, epoch 0: 3.4273760318756104\n",
      "Thoughtsformer Train Loss at batch 185, epoch 0: 3.2993876934051514\n",
      "Thoughtsformer Train Loss at batch 186, epoch 0: 3.1299757957458496\n",
      "Thoughtsformer Train Loss at batch 187, epoch 0: 3.3991761207580566\n",
      "Thoughtsformer Train Loss at batch 188, epoch 0: 3.2320780754089355\n",
      "Thoughtsformer Train Loss at batch 189, epoch 0: 3.422837018966675\n",
      "Thoughtsformer Train Loss at batch 190, epoch 0: 3.3017771244049072\n",
      "Thoughtsformer Train Loss at batch 191, epoch 0: 3.2778849601745605\n",
      "Thoughtsformer Train Loss at batch 192, epoch 0: 3.305114269256592\n",
      "Thoughtsformer Train Loss at batch 193, epoch 0: 3.218111515045166\n",
      "Thoughtsformer Train Loss at batch 194, epoch 0: 3.381488561630249\n",
      "Thoughtsformer Train Loss at batch 195, epoch 0: 3.272693157196045\n",
      "Thoughtsformer Train Loss at batch 196, epoch 0: 3.330376625061035\n",
      "Thoughtsformer Train Loss at batch 197, epoch 0: 3.326442241668701\n",
      "Thoughtsformer Train Loss at batch 198, epoch 0: 3.334887981414795\n",
      "Thoughtsformer Train Loss at batch 199, epoch 0: 3.303363561630249\n",
      "Thoughtsformer Train Loss at batch 200, epoch 0: 3.3068583011627197\n",
      "Thoughtsformer Train Loss at batch 201, epoch 0: 3.1494815349578857\n",
      "Thoughtsformer Train Loss at batch 202, epoch 0: 3.239671230316162\n",
      "Thoughtsformer Train Loss at batch 203, epoch 0: 3.297766923904419\n",
      "Thoughtsformer Train Loss at batch 204, epoch 0: 3.370208263397217\n",
      "Thoughtsformer Train Loss at batch 205, epoch 0: 3.335594654083252\n",
      "Thoughtsformer Train Loss at batch 206, epoch 0: 3.4237258434295654\n",
      "Thoughtsformer Train Loss at batch 207, epoch 0: 3.4671852588653564\n",
      "Thoughtsformer Train Loss at batch 208, epoch 0: 3.3747711181640625\n",
      "Thoughtsformer Train Loss at batch 209, epoch 0: 3.346078395843506\n",
      "Thoughtsformer Train Loss at batch 210, epoch 0: 3.350855827331543\n",
      "Thoughtsformer Train Loss at batch 211, epoch 0: 3.2729239463806152\n",
      "Thoughtsformer Train Loss at batch 212, epoch 0: 3.243542432785034\n",
      "Thoughtsformer Train Loss at batch 213, epoch 0: 3.162997245788574\n",
      "Thoughtsformer Train Loss at batch 214, epoch 0: 3.3604865074157715\n",
      "Thoughtsformer Train Loss at batch 215, epoch 0: 3.322159767150879\n",
      "Thoughtsformer Train Loss at batch 216, epoch 0: 3.4627561569213867\n",
      "Thoughtsformer Train Loss at batch 217, epoch 0: 3.1228601932525635\n",
      "Thoughtsformer Train Loss at batch 218, epoch 0: 3.3001933097839355\n",
      "Thoughtsformer Train Loss at batch 219, epoch 0: 3.179004430770874\n",
      "Thoughtsformer Train Loss at batch 220, epoch 0: 3.3103222846984863\n",
      "Thoughtsformer Train Loss at batch 221, epoch 0: 3.2464113235473633\n",
      "Thoughtsformer Train Loss at batch 222, epoch 0: 3.2122926712036133\n",
      "Thoughtsformer Train Loss at batch 223, epoch 0: 3.1598143577575684\n",
      "Thoughtsformer Train Loss at batch 224, epoch 0: 3.4241840839385986\n",
      "Thoughtsformer Train Loss at batch 225, epoch 0: 3.293478488922119\n",
      "Thoughtsformer Train Loss at batch 226, epoch 0: 3.3231797218322754\n",
      "Thoughtsformer Train Loss at batch 227, epoch 0: 3.2049107551574707\n",
      "Thoughtsformer Train Loss at batch 228, epoch 0: 3.265568733215332\n",
      "Thoughtsformer Train Loss at batch 229, epoch 0: 3.3982131481170654\n",
      "Thoughtsformer Train Loss at batch 230, epoch 0: 3.358649969100952\n",
      "Thoughtsformer Train Loss at batch 231, epoch 0: 3.3163490295410156\n",
      "Thoughtsformer Train Loss at batch 232, epoch 0: 3.256411075592041\n",
      "Thoughtsformer Train Loss at batch 233, epoch 0: 3.4097561836242676\n",
      "Thoughtsformer Train Loss at batch 234, epoch 0: 3.3082525730133057\n",
      "Thoughtsformer Train Loss at batch 235, epoch 0: 3.1505074501037598\n",
      "Thoughtsformer Train Loss at batch 236, epoch 0: 3.3452463150024414\n",
      "Thoughtsformer Train Loss at batch 237, epoch 0: 3.3054416179656982\n",
      "Thoughtsformer Train Loss at batch 238, epoch 0: 3.256291389465332\n",
      "Thoughtsformer Train Loss at batch 239, epoch 0: 3.2070984840393066\n",
      "Thoughtsformer Train Loss at batch 240, epoch 0: 3.360661506652832\n",
      "Thoughtsformer Train Loss at batch 241, epoch 0: 3.2269275188446045\n",
      "Thoughtsformer Train Loss at batch 242, epoch 0: 3.22324800491333\n",
      "Thoughtsformer Train Loss at batch 243, epoch 0: 3.211061477661133\n",
      "Thoughtsformer Train Loss at batch 244, epoch 0: 3.359957695007324\n",
      "Thoughtsformer Train Loss at batch 245, epoch 0: 3.43098521232605\n",
      "Thoughtsformer Train Loss at batch 246, epoch 0: 3.3275227546691895\n",
      "Thoughtsformer Train Loss at batch 247, epoch 0: 3.1858744621276855\n",
      "Thoughtsformer Train Loss at batch 248, epoch 0: 3.2237799167633057\n",
      "Thoughtsformer Train Loss at batch 249, epoch 0: 3.169609546661377\n",
      "Thoughtsformer Train Loss at batch 250, epoch 0: 3.3446080684661865\n",
      "Thoughtsformer Train Loss at batch 251, epoch 0: 3.145880699157715\n",
      "Thoughtsformer Train Loss at batch 252, epoch 0: 3.3709235191345215\n",
      "Thoughtsformer Train Loss at batch 253, epoch 0: 3.1863772869110107\n",
      "Thoughtsformer Train Loss at batch 254, epoch 0: 3.380923271179199\n",
      "Thoughtsformer Train Loss at batch 255, epoch 0: 3.2798445224761963\n",
      "Thoughtsformer Train Loss at batch 256, epoch 0: 3.1907215118408203\n",
      "Thoughtsformer Train Loss at batch 257, epoch 0: 3.266690731048584\n",
      "Thoughtsformer Train Loss at batch 258, epoch 0: 3.382444381713867\n",
      "Thoughtsformer Train Loss at batch 259, epoch 0: 3.2202773094177246\n",
      "Thoughtsformer Train Loss at batch 260, epoch 0: 3.258434772491455\n",
      "Thoughtsformer Train Loss at batch 261, epoch 0: 3.163800001144409\n",
      "Thoughtsformer Train Loss at batch 262, epoch 0: 3.3456287384033203\n",
      "Thoughtsformer Train Loss at batch 263, epoch 0: 3.267169952392578\n",
      "Thoughtsformer Train Loss at batch 264, epoch 0: 3.362185478210449\n",
      "Thoughtsformer Train Loss at batch 265, epoch 0: 3.169124126434326\n",
      "Thoughtsformer Train Loss at batch 266, epoch 0: 3.4201645851135254\n",
      "Thoughtsformer Train Loss at batch 267, epoch 0: 3.1935362815856934\n",
      "Thoughtsformer Train Loss at batch 268, epoch 0: 3.274240493774414\n",
      "Thoughtsformer Train Loss at batch 269, epoch 0: 3.2633776664733887\n",
      "Thoughtsformer Train Loss at batch 270, epoch 0: 3.1723737716674805\n",
      "Thoughtsformer Train Loss at batch 271, epoch 0: 3.2541697025299072\n",
      "Thoughtsformer Train Loss at batch 272, epoch 0: 3.302218198776245\n",
      "Thoughtsformer Train Loss at batch 273, epoch 0: 3.3850228786468506\n",
      "Thoughtsformer Train Loss at batch 274, epoch 0: 3.4320592880249023\n",
      "Thoughtsformer Train Loss at batch 275, epoch 0: 3.2058541774749756\n",
      "Thoughtsformer Train Loss at batch 276, epoch 0: 3.2351975440979004\n",
      "Thoughtsformer Train Loss at batch 277, epoch 0: 3.305251121520996\n",
      "Thoughtsformer Train Loss at batch 278, epoch 0: 3.37276554107666\n",
      "Thoughtsformer Train Loss at batch 279, epoch 0: 3.3551266193389893\n",
      "Thoughtsformer Train Loss at batch 280, epoch 0: 3.3063910007476807\n",
      "Thoughtsformer Train Loss at batch 281, epoch 0: 3.300374984741211\n",
      "Thoughtsformer Train Loss at batch 282, epoch 0: 3.2888667583465576\n",
      "Thoughtsformer Train Loss at batch 283, epoch 0: 3.3037679195404053\n",
      "Thoughtsformer Train Loss at batch 284, epoch 0: 3.2825419902801514\n",
      "Thoughtsformer Train Loss at batch 285, epoch 0: 3.392287254333496\n",
      "Thoughtsformer Train Loss at batch 286, epoch 0: 3.3098442554473877\n",
      "Thoughtsformer Train Loss at batch 287, epoch 0: 3.3936846256256104\n",
      "Thoughtsformer Train Loss at batch 288, epoch 0: 3.2736501693725586\n",
      "Thoughtsformer Train Loss at batch 289, epoch 0: 3.265820026397705\n",
      "Thoughtsformer Train Loss at batch 290, epoch 0: 3.2768912315368652\n",
      "Thoughtsformer Train Loss at batch 291, epoch 0: 3.254035234451294\n",
      "Thoughtsformer Train Loss at batch 292, epoch 0: 3.1912572383880615\n",
      "Thoughtsformer Train Loss at batch 293, epoch 0: 3.226574659347534\n",
      "Thoughtsformer Train Loss at batch 294, epoch 0: 3.327831745147705\n",
      "Thoughtsformer Train Loss at batch 295, epoch 0: 3.266371250152588\n",
      "Thoughtsformer Train Loss at batch 296, epoch 0: 3.2606537342071533\n",
      "Thoughtsformer Train Loss at batch 297, epoch 0: 3.3255085945129395\n",
      "Thoughtsformer Train Loss at batch 298, epoch 0: 3.3912575244903564\n",
      "Thoughtsformer Train Loss at batch 299, epoch 0: 3.2589759826660156\n",
      "Thoughtsformer Train Loss at batch 300, epoch 0: 3.3861823081970215\n",
      "Thoughtsformer Train Loss at batch 301, epoch 0: 3.3386101722717285\n",
      "Thoughtsformer Train Loss at batch 302, epoch 0: 3.2008254528045654\n",
      "Thoughtsformer Train Loss at batch 303, epoch 0: 3.2260968685150146\n",
      "Thoughtsformer Train Loss at batch 304, epoch 0: 3.2792184352874756\n",
      "Thoughtsformer Train Loss at batch 305, epoch 0: 3.263810157775879\n",
      "Thoughtsformer Train Loss at batch 306, epoch 0: 3.286963939666748\n",
      "Thoughtsformer Train Loss at batch 307, epoch 0: 3.31545352935791\n",
      "Thoughtsformer Train Loss at batch 308, epoch 0: 3.160621166229248\n",
      "Thoughtsformer Train Loss at batch 309, epoch 0: 3.1727206707000732\n",
      "Thoughtsformer Train Loss at batch 310, epoch 0: 3.3888118267059326\n",
      "Thoughtsformer Train Loss at batch 311, epoch 0: 3.273554801940918\n",
      "Thoughtsformer Train Loss at batch 312, epoch 0: 3.2452640533447266\n",
      "Thoughtsformer Train Loss at batch 313, epoch 0: 3.260993242263794\n",
      "Thoughtsformer Train Loss at batch 314, epoch 0: 3.291748046875\n",
      "Thoughtsformer Train Loss at batch 315, epoch 0: 3.190307855606079\n",
      "Thoughtsformer Train Loss at batch 316, epoch 0: 3.206923484802246\n",
      "Thoughtsformer Train Loss at batch 317, epoch 0: 3.2630205154418945\n",
      "Thoughtsformer Train Loss at batch 318, epoch 0: 3.371547222137451\n",
      "Thoughtsformer Train Loss at batch 319, epoch 0: 3.233642101287842\n",
      "Thoughtsformer Train Loss at batch 320, epoch 0: 3.1610822677612305\n",
      "Thoughtsformer Train Loss at batch 321, epoch 0: 3.1846680641174316\n",
      "Thoughtsformer Train Loss at batch 322, epoch 0: 3.3785743713378906\n",
      "Thoughtsformer Train Loss at batch 323, epoch 0: 3.213181972503662\n",
      "Thoughtsformer Train Loss at batch 324, epoch 0: 3.2437429428100586\n",
      "Thoughtsformer Train Loss at batch 325, epoch 0: 3.271224021911621\n",
      "Thoughtsformer Train Loss at batch 326, epoch 0: 3.252448081970215\n",
      "Thoughtsformer Train Loss at batch 327, epoch 0: 3.1940016746520996\n",
      "Thoughtsformer Train Loss at batch 328, epoch 0: 3.275750160217285\n",
      "Thoughtsformer Train Loss at batch 329, epoch 0: 3.1653354167938232\n",
      "Thoughtsformer Train Loss at batch 330, epoch 0: 3.2356348037719727\n",
      "Thoughtsformer Train Loss at batch 331, epoch 0: 3.288430690765381\n",
      "Thoughtsformer Train Loss at batch 332, epoch 0: 3.2562592029571533\n",
      "Thoughtsformer Train Loss at batch 333, epoch 0: 3.356048345565796\n",
      "Thoughtsformer Train Loss at batch 334, epoch 0: 3.1934189796447754\n",
      "Thoughtsformer Train Loss at batch 335, epoch 0: 3.2786192893981934\n",
      "Thoughtsformer Train Loss at batch 336, epoch 0: 3.2946205139160156\n",
      "Thoughtsformer Train Loss at batch 337, epoch 0: 3.323643207550049\n",
      "Thoughtsformer Train Loss at batch 338, epoch 0: 3.46077823638916\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "models = [m2]\n",
    "loss_fn = F.cross_entropy\n",
    "optims = [torch.optim.Adam(params=m.parameters(), lr=0.0003) for m in models]\n",
    "\n",
    "\n",
    "# Assume your train_loader provides input tensors of shape [batch_size, 1000, d_embed]\n",
    "losses_over_time = [ [] for _ in models]\n",
    "test_losses_over_time = [ [] for _ in models]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    [m.train() for m in models]\n",
    "    for idx, (tokens, labels) in enumerate(train_loader):\n",
    "        batch_size, sequence_length = tokens.shape\n",
    "\n",
    "\n",
    "        # Create padding mask (no padding here, but adding for future flexibility)\n",
    "        padding_mask = torch.zeros(batch_size, sequence_length).to(device) # additional padding is done internally\n",
    "\n",
    "        tokens = tokens.to(device)\n",
    "        # Forward pass through the model\n",
    "        ls_logits = [m(tokens, padding_mask) for m in models]\n",
    "        # print(thoughts_logits.shape)\n",
    "        thoughts_loss_ls = [loss_fn(logit.permute(0, 2, 1), labels.to(device)) for logit in ls_logits]\n",
    "        [loss_ls.append(thought_loss.item()) for thought_loss, loss_ls in zip(thoughts_loss_ls, losses_over_time)]\n",
    "\n",
    "        [optim.zero_grad() for optim in optims]\n",
    "        [loss.backward() for loss in thoughts_loss_ls]\n",
    "        [optim.step() for optim in optims]\n",
    "\n",
    "        [print(f\"Thoughtsformer Train Loss at batch {idx}, epoch {epoch}: {loss.item()}\") for loss in thoughts_loss_ls]\n",
    "\n",
    "    # Validate the model on the test set after each epoch\n",
    "    [m.eval() for m in models]  # Set model to evaluation mode\n",
    "    test_losses = [0 for _ in models]\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for idx, (tokens, labels) in enumerate(test_loader):\n",
    "            batch_size, sequence_length = tokens.shape\n",
    "\n",
    "\n",
    "            # Create padding mask (no padding here, but adding for future flexibility)\n",
    "            padding_mask = torch.zeros(batch_size, sequence_length).to(device) # additional padding is done internally\n",
    "            tokens = tokens.to(device)\n",
    "            # Forward pass through the model\n",
    "            ls_logits = [m(tokens, padding_mask) for m in models]\n",
    "            test_loss_ls = [loss_fn(logit.permute(0, 2, 1), labels.to(device)) for logit in ls_logits]\n",
    "        \n",
    "            for i in range(len(test_losses)):\n",
    "                test_losses[i] += test_loss_ls[i].item()\n",
    "\n",
    "    avg_test_losses = [test_loss / len(test_loader) for test_loss in test_losses]\n",
    "    [test_loss_over_time.append(avg_test_losses) for avg_test_loss, test_loss_over_time in zip(avg_test_losses, test_losses_over_time)]\n",
    "    [print(f\"Test Loss after epoch {epoch}: {avg_test_loss}\") for avg_test_loss in avg_test_losses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bicke\\AppData\\Local\\Temp\\ipykernel_16812\\1672811808.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test = test.load_state_dict(torch.load(\"saves/possibly_flawed_transformer.pth\"))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thoughtsformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
