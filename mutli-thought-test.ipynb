{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from thoughtsformer import ThoughtsFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tiny_shakespeare import TinyShakespeareDataset, TokenizerType\n",
    "from character_tokenizer import ShakespeareCharacterTokenizer\n",
    "dataset_tr = TinyShakespeareDataset(128,64, split=\"train\", tokenizer=TokenizerType.CHARACTER_LEVEL)\n",
    "dataset_te = TinyShakespeareDataset(128,64, split=\"test\", tokenizer=TokenizerType.CHARACTER_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "max_sequence_length = 128\n",
    "vocab_size = 65\n",
    "num_layers = 1\n",
    "n_head = 6\n",
    "d_embed = 384\n",
    "dim_feedforward = d_embed\n",
    "dropout = 0\n",
    "\n",
    "def return_model_with_n_thoughts(n: int):\n",
    "  return ThoughtsFormer(\n",
    "      max_thought_len=n,\n",
    "      max_sequence_length=max_sequence_length,\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      n_head=n_head,\n",
    "      d_embed=d_embed,\n",
    "      dim_feedforward=dim_feedforward,\n",
    "      dropout=dropout\n",
    "  ).to(device)\n",
    "\n",
    "m0 = return_model_with_n_thoughts(0)\n",
    "m1 = return_model_with_n_thoughts(3)\n",
    "m2 = return_model_with_n_thoughts(4)\n",
    "m3 = return_model_with_n_thoughts(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dataset = TinyShakespeareDataset(512,64)\n",
    "train_size = len(dataset_tr)\n",
    "test_size = len(dataset_te) \n",
    "\n",
    "train_loader = DataLoader(dataset_tr, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(dataset_te, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bicke\\anaconda3\\envs\\thoughtsformer\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "models = [m2]\n",
    "loss_fn = F.cross_entropy\n",
    "optims = [torch.optim.Adam(params=m.parameters(), lr=0.001) for m in models]\n",
    "\n",
    "\n",
    "# Assume your train_loader provides input tensors of shape [batch_size, 1000, d_embed]\n",
    "losses_over_time = [ [] for _ in models]\n",
    "test_losses_over_time = [ [] for _ in models]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    [m.train() for m in models]\n",
    "    for idx, (tokens, labels) in enumerate(train_loader):\n",
    "        batch_size, sequence_length = tokens.shape\n",
    "\n",
    "\n",
    "        # Create padding mask (no padding here, but adding for future flexibility)\n",
    "        padding_mask = torch.zeros(batch_size, sequence_length).to(device) # additional padding is done internally\n",
    "\n",
    "        tokens = tokens.to(device)\n",
    "        # Forward pass through the model\n",
    "        ls_logits = [m(tokens, padding_mask) for m in models]\n",
    "        # print(thoughts_logits.shape)\n",
    "        thoughts_loss_ls = [loss_fn(logit.permute(0, 2, 1), labels.to(device)) for logit in ls_logits]\n",
    "        [loss_ls.append(thought_loss.item()) for thought_loss, loss_ls in zip(thoughts_loss_ls, losses_over_time)]\n",
    "\n",
    "        [optim.zero_grad() for optim in optims]\n",
    "        [loss.backward() for loss in thoughts_loss_ls]\n",
    "        [optim.step() for optim in optims]\n",
    "\n",
    "        [print(f\"Thoughtsformer Train Loss at batch {idx}, epoch {epoch}: {loss.item()}\") for loss in thoughts_loss_ls]\n",
    "\n",
    "    # Validate the model on the test set after each epoch\n",
    "    [m.eval() for m in models]  # Set model to evaluation mode\n",
    "    test_losses = [0 for _ in models]\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for idx, (tokens, labels) in enumerate(test_loader):\n",
    "            batch_size, sequence_length = tokens.shape\n",
    "\n",
    "\n",
    "            # Create padding mask (no padding here, but adding for future flexibility)\n",
    "            padding_mask = torch.zeros(batch_size, sequence_length).to(device) # additional padding is done internally\n",
    "            tokens = tokens.to(device)\n",
    "            # Forward pass through the model\n",
    "            ls_logits = [m(tokens, padding_mask) for m in models]\n",
    "            test_loss_ls = [loss_fn(logit.permute(0, 2, 1), labels.to(device)) for logit in ls_logits]\n",
    "        \n",
    "            for i in range(len(test_losses)):\n",
    "                test_losses[i] += test_loss_ls[i].item()\n",
    "\n",
    "    avg_test_losses = [test_loss / len(test_loader) for test_loss in test_losses]\n",
    "    [test_loss_over_time.append(avg_test_losses) for avg_test_loss, test_loss_over_time in zip(avg_test_losses, test_losses_over_time)]\n",
    "    [print(f\"Test Loss after epoch {epoch}: {avg_test_loss}\") for avg_test_loss in avg_test_losses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backward_memory_in_gb(model: torch.nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Calculate memory used by gradients during backward pass.\n",
    "    Call this after loss.backward() to get memory usage.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model after backward pass\n",
    "    \n",
    "    Returns:\n",
    "        Memory usage in GB\n",
    "    \"\"\"\n",
    "    total_memory = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            # Each gradient is stored in float32 (4 bytes)\n",
    "            total_memory += param.grad.nelement() * param.grad.element_size()\n",
    "            \n",
    "            # For BPTT, account for gradient accumulation buffers\n",
    "            if hasattr(param, '_grad_accumulator'):\n",
    "                total_memory += param.grad.nelement() * param.grad.element_size()\n",
    "    \n",
    "    return total_memory / (1024**3)  # Convert bytes to GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bicke\\AppData\\Local\\Temp\\ipykernel_16812\\1672811808.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test = test.load_state_dict(torch.load(\"saves/possibly_flawed_transformer.pth\"))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thoughtsformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
